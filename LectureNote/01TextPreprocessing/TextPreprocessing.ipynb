{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "- Tokens and Normalization Techniques \n",
    "- JSON, CSV, TXT handling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b940ec1336ef55a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Natural Language Processing Overview\n",
    "\n",
    "**◆ The difference between structured and unstructured data**\n",
    " \n",
    "Structured data consists of subjects, variables, and observations, and they are written in a structure (spreadsheet) suitable for general statistical analysis. Unstructured data are documents, images, audio, video, etc., and it is difficult to apply traditiona statistical analysis techniques, and pre-processing is required for analysis.\n",
    "\n",
    "\n",
    "**◆ Difficulty of natural language (text) processing**\n",
    "\n",
    "Text, a representative example of unstructured data, is composed of natural language, and text corresponds to data encoded in natural language. Since natural language has been created and changed with various usages, it has characteristics such as ambiguity, relevance, and ambiguity, and it is difficult to process with one rule because it dynamically changes meaning according to viewpoints and times. Natural language is difficult to analyze because it can be interpreted differently depending on the meaning or context even if the preprocessing process is well done. Sometimes, analysis considering emoticons( encoding of emotional expression ) is necessary, and recently, emoji symbols are also actively used as an extension concept of natural language\n",
    "\n",
    "**◆ corpus**\n",
    "natural language A collection of related documents contained in the \n",
    "\n",
    "> corpus > paragraph > syntax or sentence > word\n",
    "> word = syllabus + phonemes + affixes + letters\n",
    "\n",
    "also includes metadata ( information about the document , such as author and date of creation ), so these parts must be distinguished when reading the corpus.\n",
    "\n",
    "**◆ token**\n",
    "\n",
    "The unit of text analysis is called a token , and a token is a string encoded in computer binary numbe rs (bytes) to represent text. A token can be a word or a sentence.\n",
    "\n",
    "**◆ Korean Tokenization**\n",
    "\n",
    "Unlike English, Korean has postpositional particles, so you must understand the concept of morpheme. (Usally, Korean language analysis is more complicated than english)\n",
    "Two morphemes :\n",
    "(1) Independent morphemes : self-reliant as words such as\n",
    "(2) Dependent morphemes : morphemes used in combination with other morphemes, such as\n",
    "tokenization of words rather than word tokenization to obtain tokenization similar to English necessary Part -of-speech tagging needs to check which parts of speech\n",
    "\n",
    "**◆ bag of words model**\n",
    "\n",
    "The word is evaluated how often it occurs with other words in a particular situation. See which words are interlocked and appear at the same time.\n",
    "\n",
    "**stop words** : Stop words are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. There is no single universal list of stop words used by all natural language processing tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list. Therefore, any group of words can be chosen as the stop words for a given purpose. such as 'the','a','and', etc...\n",
    "\n",
    "**◆ (n-gram) analysis**\n",
    "\n",
    "By specifying the number of characters or words to be entered as n instead of 1 according to the sequence, it helps to understand the context.\n",
    "\n",
    "Why we use n-gram? due to understand 'context'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fdd87bbdc94d2d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preprocessing step\n",
    "\n",
    "**◆ Refine**\n",
    "\n",
    "Required before and after tokenization as a denoising step. When HTML is parsed, tag names are rem oved, and stopwords and special characters are removed after tokenization.\n",
    "\n",
    "**◆ Tokenization**\n",
    "\n",
    "Tokenize words or sentences by breaking text into desired unit.\n",
    "\n",
    "**◆ Normalization**\n",
    "\n",
    "A work that unifies words written in different forms into standard words. Normalization methods incl ude stem extraction and lemma extraction.\n",
    "\n",
    "**◆ Part-of-speech tagging**\n",
    "\n",
    "Tokenized words are marked by classifying parts of speech, and the same word can have different contexts depending on parts of speech, so it is necessary to distinguish the.\n",
    "\n",
    "#### Regular Expression\n",
    "\n",
    "regular expression is regular expression Or an expression that uses a type of formal language grammar called a regular grammar.\n",
    "Regular expressions are also called lock language. The lock judges whether a given lock language sentence matches a particularly meaningful sentence and presents an appropriate response."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25c94400bfb5e070"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ The re module**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f52cfd46304bcfb2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-18T07:50:12.249864Z",
     "start_time": "2023-09-18T07:50:12.241258Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile('[a-z]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function `re.compile()` Compile the regular expression using , and work with the compiled object p."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d90f656c60df38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(1) String search using regular expression \n",
    "\n",
    "- `match()`: Matches the regular expression\n",
    "- `search()`: Searches\n",
    "- `findall()`: returns all strings (substrings) that match the regular expression return as list \n",
    "- `finditer()`: returns an\n",
    "\n",
    "(2) method of match object \n",
    "\n",
    "- `group()` : returns\n",
    "- `start()` : of the matched string return \n",
    "- `end()`: returns\n",
    "- `span()`: returns\n",
    "\n",
    "(3) Compilation option : used as \n",
    "- `re.DOTALL` or `re.S` : allows\n",
    "- `re.IGNORE` or `re.I` : allows\n",
    "- `re.MULTILINE` or `re.M` : allows matching against\n",
    "- `re.VERBOSE` or `re.X` : to make the regex easier to see and to use comments etc.\n",
    "\n",
    "(4) grouping\n",
    "- `group(0)` : the entire matched string\n",
    "- `group(n)` : String corresponding to the nth group"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f111bedd6217525"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ Meta character**\n",
    "\n",
    "A character used for a separate purpose other than the original meaning of the character\n",
    "\n",
    "\\.\\^\\$\\*\\+\\?\\{\\}\\[\\]\\\\\\|\\(\\) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faed3f9b69185e17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(1) character class [ ], \\  <br>\n",
    "\n",
    "in [ ] Matches characters . However , using ^ in [ ] means not .\n",
    "special When using letters of meaning , uppercase letters mean the opposite of lowercase letters.\n",
    "\n",
    "(2) dot(.)\n",
    "\n",
    "It matches with all letters except for the newline character \\n\n",
    "Ex) a.b : between a and b all characters Note) a[.]b : dot(.) between a and b\n",
    "\n",
    "(3) Repetition (*): The character preceding \\* is repeated \n",
    "\n",
    "Ex) ca*t : ct , cat , caat all applicable\n",
    "\n",
    "(4) Repeat (+): Matches the character before the + repeated\n",
    "\n",
    "(5) repeat ({ m,n }, ? ):\n",
    "\n",
    "{m} : immediately Matches the previous character m times Ex ) do{2}g : doog\n",
    "\n",
    "{m, n} : directly Matching the preceding character from number m to number n is permitted\n",
    "- Ex) do{2,3}g : doog , dooog \n",
    "\n",
    "? : Same meaning as {0,1}\n",
    "\n",
    "(6) Backslash ( \\ ) : Backslash Escape by using twice of backslash\n",
    "\n",
    "Ex) \\ \\section : Search for a string called section. If \\ section is used , it is recognized as \\s of the character class.\n",
    "\n",
    "(7) or ( | ): Ex) a|b : a or b\n",
    "\n",
    "(8) Matching at the beginning of a string ( ^ ): same as\n",
    "\n",
    "Ex) ^My : My to string Matches only if it is at the beginning\n",
    "\n",
    "(9) Matches end of string ( $ ) : Same as\n",
    "\n",
    "Ex) strong$: matches only when strong is located at the end of the string\n",
    "\n",
    "(10) Grouping ( ): Create a group. \n",
    "\n",
    "Ex) ( \\w+) \\ s \\d+ : extracts only"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86f8186f1d5a7556"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77ce524a2c8092f7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('webtext') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T08:43:53.068827Z",
     "start_time": "2023-09-18T08:43:46.269239Z"
    }
   },
   "id": "9eeaa19f09ca9919"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. toekn split**\n",
    "\n",
    "(1) one Split a sentence (split(): Convert a string to a list )\n",
    "\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of twenty-six.\"\"\" \n",
    "\n",
    "`sentence.split ()`\n",
    "(Three quotes are just newlines Line wrapping output possible : Try using `print()`)\n",
    "\n",
    "(2) Can be extended to a corpus composed of one or more sentences : Paragraphs are divided into sentences , and each s entence is tokenized. corpus = [\"Thomas Jefferson began building Monticello at the age of twenty-six.\",\n",
    "\"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "\"Wondering, she opened the door to the studio.\" ] corpus_split = [x.split() for x in corpus]\n",
    "\n",
    "- tokenize.sent_tokenize(p) 문장으로 분할 \n",
    "- tokenize.word_tokenize(p) 단어로 분할 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "192b152832a0c1b1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "p = \"She is a heroine. He is an AI developer. Don't be late, Mr. Kim's house is: very-near here!\"\n",
    "tokenize.sent_tokenize(p) # Ctrl+Shif+Enter\n",
    "tokenize.word_tokenize(p) # Don't과 Kim's ?\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text = '''She is a heroine. He is an AI developer. Don't be late, Mr. Kim's house is: very-near here!'''\n",
    "tokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:18:47.614269Z",
     "start_time": "2023-09-20T08:18:47.121668Z"
    }
   },
   "id": "a12da6a3983099f6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'is', 'a', 'heroine', '.', 'He', 'is', 'an', 'AI', 'developer', '.', 'Do', \"n't\", 'be', 'late', ',', 'Mr.', 'Kim', \"'s\", 'house', 'is', ':', 'very-near', 'here', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:18:58.530589Z",
     "start_time": "2023-09-20T08:18:58.521795Z"
    }
   },
   "id": "5e4c2042a91266c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Token Frequency analysis**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfd20d70b673651c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({'Good': 1, 'morning,': 1, 'Kim': 1})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "corpus_split = Counter(\"Good morning, Kim\".split()) \n",
    "#Counter(\"Good morning, Kim!\".split()) \n",
    "Counter(corpus_split) # use"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:20:08.846119Z",
     "start_time": "2023-09-20T08:20:08.838153Z"
    }
   },
   "id": "a6990cd51d2ab9b6"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'is': 3, '.': 2, 'She': 1, 'a': 1, 'heroine': 1, 'He': 1, 'an': 1, 'AI': 1, 'developer': 1, 'Do': 1, \"n't\": 1, 'be': 1, 'late': 1, ',': 1, 'Mr.': 1, 'Kim': 1, \"'s\": 1, 'house': 1, ':': 1, 'very-near': 1, 'here': 1, '!': 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(tokens)) # 반환은 사전 형태이다. 불용어도 없어지면 더 사용하기 좋을 듯 하다"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:20:33.565500Z",
     "start_time": "2023-09-20T08:20:33.557334Z"
    }
   },
   "id": "e6966c64dd71522e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# of words combination : Modification of order (translation of meaning is possible in relation to grammar)\n",
    "from itertools import permutations\n",
    "[\" \".join(combo) for combo in permutations('Good morning Rosa!'.split(), 3)]\n",
    "s = \"Find textbooks with titles containing 'NLP', or 'natural' and 'language'.\"\n",
    "len(set(s.split()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:21:22.924408Z",
     "start_time": "2023-09-20T08:21:22.918027Z"
    }
   },
   "id": "d5eeeb1e11fd52f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Note) \" \". join(A): Convert A (consisting of multiple elements) in list form into a string in the form of jumping (each element)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77e5092f7a44ff1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Regular Expressions**\n",
    "\n",
    "(1) String search using regular expression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b79851ca9c4efe0"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile('[az]+') # character repeated at least 1 time\n",
    "\n",
    "m = p.match('python') # match: If the first value of the string matches, return the result \n",
    "print(m)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:30:57.569823Z",
     "start_time": "2023-09-20T08:30:57.562145Z"
    }
   },
   "id": "8a4add2fc29f5a1e"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroup\u001B[49m() \u001B[38;5;66;03m# group() : return match string\u001B[39;00m\n\u001B[1;32m      2\u001B[0m m\u001B[38;5;241m.\u001B[39mstart() \u001B[38;5;66;03m# start() : start position of match string\u001B[39;00m\n\u001B[1;32m      3\u001B[0m m\u001B[38;5;241m.\u001B[39mend() \u001B[38;5;66;03m# end(): end position of match string\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "m.group() # group() : return match string\n",
    "m.start() # start() : start position of match string\n",
    "m.end() # end(): end position of match string\n",
    "m.span() # span(): ( start , end ) position of match string\n",
    "\n",
    "m1 = p.match(\"3 python\") # match: because the first value of the string is a number None output \n",
    "print(m1)\n",
    "\n",
    "p.search(\"python\").group() # search: find matching value in entire string and return if found\n",
    "\n",
    "print(p. findall(\"life is too short\")) # findall : returns a list of all strings matching the regular expression p.findall(\"life is too short\").group() # ERROR : cannot use group in list\n",
    "print(p. finditer (\"life is too short\")) # finditer : similar to findall , but returns an object"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:31:08.165123Z",
     "start_time": "2023-09-20T08:31:08.145139Z"
    }
   },
   "id": "bf61cc330cdd193"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(2) compile option"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5455062b56eb7317"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python one']\n"
     ]
    }
   ],
   "source": [
    "p = re.compile('a.b', re.DOTALL) \n",
    "p.match('a\\nb').group()\n",
    "p = re.compile('[a-z]', re.I) \n",
    "p.match('PyThon').group() \n",
    "p.match('pYTHON').group()\n",
    "#^python\\s\\w+ must ( on each line ) start with the string python, followed by whitespace, followed by a word \n",
    "\n",
    "p = re.compile(\"^python\\s\\w+\", re.MULTILINE)\n",
    "data = \"\"\"python one life is too short python two you need python python three\"\"\"\n",
    "print(p.findall(data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:32:31.366056Z",
     "start_time": "2023-09-20T08:32:31.353677Z"
    }
   },
   "id": "df37b63208fee5af"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python one', 'python two', 'python three']\n"
     ]
    }
   ],
   "source": [
    "data = \"\"\"python one \n",
    "life is too short \n",
    "python two \n",
    "you need python \n",
    "python three\"\"\" \n",
    "print(p.findall(data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:33:02.491480Z",
     "start_time": "2023-09-20T08:33:02.483891Z"
    }
   },
   "id": "a047898b841699b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(3) compile and string search together"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9cb2d6a06232dbb"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile('Crow|Servo')\n",
    "p.match('CrowHello').group()\n",
    "re.search('^Life', 'Life is too short').group()\n",
    "re.search('short$', 'Life is too short').group()\n",
    "print( re.search ('short$', 'Life is too short, you need python'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:34:25.804735Z",
     "start_time": "2023-09-20T08:34:25.793366Z"
    }
   },
   "id": "1502d3614b7c814e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(4) grouping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "692f29a17dc68554"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "'ABCABCABC'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile(\"(ABC)+\")\n",
    "p.search('ABCABCABC OK?').group()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:36:02.618862Z",
     "start_time": "2023-09-20T08:36:02.596450Z"
    }
   },
   "id": "8248c33d8934fb5f"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "'park 010-1234-1234'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r\"\\w+\\s+\\d+[-]\\d+[-]\\d+\")\n",
    "p.search(\"park 010-1234-1234\").group()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:36:19.037373Z",
     "start_time": "2023-09-20T08:36:19.013102Z"
    }
   },
   "id": "f6f1e5f767bd70da"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "'park'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r\"(\\w+)\\s+(\\d+[-]\\d+[-]\\d+)\") \n",
    "p.search(\"park 010-1234-1234\").group(1) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:36:55.635273Z",
     "start_time": "2023-09-20T08:36:55.619692Z"
    }
   },
   "id": "1f398679e9a67b78"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "'010-1234-1234'"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.search(\"park 010-1234-1234\").group(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:36:57.306344Z",
     "start_time": "2023-09-20T08:36:57.283857Z"
    }
   },
   "id": "f30ef2467fe4d811"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(5) Forward searching"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd9432e87b1c0d3e"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "'http:'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile(\".+:\")\n",
    "p.search(\"http://google.com\").group()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:37:49.046810Z",
     "start_time": "2023-09-20T08:37:49.020432Z"
    }
   },
   "id": "4e22b59c03377d0"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "'http'"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\".+(?=:)\")\n",
    "p.search(\"http://google.com\").group()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:38:23.849069Z",
     "start_time": "2023-09-20T08:38:23.823768Z"
    }
   },
   "id": "7b7ff29aa26ab575"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(6) String replacement"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc06a72f72dca11a"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "('colour sucks and colour shoes', 2)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile('(blue|white|red)')\n",
    "p.subn('colour','blue sucks and red shoes')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:39:49.307802Z",
     "start_time": "2023-09-20T08:39:49.280414Z"
    }
   },
   "id": "6fcc3b4be783f5b2"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 32)\n"
     ]
    }
   ],
   "source": [
    "s = '<html><head><title>Title</title>'\n",
    "print(re.match('<.*>', s).span())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:40:11.867891Z",
     "start_time": "2023-09-20T08:40:11.838868Z"
    }
   },
   "id": "7ee7a22b5107fef3"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><title>Title</title>\n"
     ]
    }
   ],
   "source": [
    "print(re.match('<.*>', s).group())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:40:25.792328Z",
     "start_time": "2023-09-20T08:40:25.770506Z"
    }
   },
   "id": "c065ca994a9653b4"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n"
     ]
    }
   ],
   "source": [
    "print(re.match('<.*?>', s).group())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:40:31.154983Z",
     "start_time": "2023-09-20T08:40:31.134359Z"
    }
   },
   "id": "65171bb8fccc0fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(7) HTML tag remove"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c74853a55c127a09"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "import re\n",
    "html = (\"<html><head>some header information</head> \\\n",
    "<Body>it's start. <script src='..'>some script</script> \\\n",
    "<!-- some comments -->some <b>body</b> contents.. \\n <a href ='some link'> gogo </a> \\\n",
    "and other stuff .. <script>another</script></Body></html>\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:23.576447Z",
     "start_time": "2023-09-20T08:43:23.560227Z"
    }
   },
   "id": "8ed3529096407a9a"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "str"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(html)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:23.900498Z",
     "start_time": "2023-09-20T08:43:23.886359Z"
    }
   },
   "id": "7c31f0900f0c133"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "240"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(html)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:24.208840Z",
     "start_time": "2023-09-20T08:43:24.203495Z"
    }
   },
   "id": "3047fde9a0d47082"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "\"<Body>it's start. <script src='..'>some script</script> <!-- some comments -->some <b>body</b> contents.. \\n <a href ='some link'> gogo </a> and other stuff .. <script>another</script></Body>\""
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = re.search('<body.*/body>', html, re.I|re.S).group()\n",
    "body"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:24.506902Z",
     "start_time": "2023-09-20T08:43:24.502673Z"
    }
   },
   "id": "535b62ac92fb37bf"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "\"<Body>it's start.  <!-- some comments -->some <b>body</b> contents.. \\n <a href ='some link'> gogo </a> and other stuff .. </Body>\""
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('<script.*?>.*?</script>', '', body, 0, re.I|re.S)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:24.951252Z",
     "start_time": "2023-09-20T08:43:24.948218Z"
    }
   },
   "id": "1456505b639a031e"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's start. some script some body contents.. \n",
      "  gogo  and other stuff .. another\n"
     ]
    }
   ],
   "source": [
    "text = re.sub('<.+?>', '', body, 0, re.I|re.S) \n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:25.301525Z",
     "start_time": "2023-09-20T08:43:25.298121Z"
    }
   },
   "id": "ff4fbd7d5c2fc08d"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's start some script some body contents   gogo  and other stuff  another\n"
     ]
    }
   ],
   "source": [
    "result = re.sub('\\t|\\r|\\n|\\.', '', text) \n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:26.013491Z",
     "start_time": "2023-09-20T08:43:26.008756Z"
    }
   },
   "id": "58412923cc7f3b16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "668cfbe01e835a03"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
