{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "- Tokens and Normalization Techniques \n",
    "- JSON, CSV, TXT handling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b940ec1336ef55a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Natural Language Processing Overview\n",
    "\n",
    "**◆ The difference between structured and unstructured data**\n",
    " \n",
    "Structured data consists of subjects, variables, and observations, and they are written in a structure (spreadsheet) suitable for general statistical analysis. Unstructured data are documents, images, audio, video, etc., and it is difficult to apply traditiona statistical analysis techniques, and pre-processing is required for analysis.\n",
    "\n",
    "\n",
    "**◆ Difficulty of natural language (text) processing**\n",
    "\n",
    "Text, a representative example of unstructured data, is composed of natural language, and text corresponds to data encoded in natural language. Since natural language has been created and changed with various usages, it has characteristics such as ambiguity, relevance, and ambiguity, and it is difficult to process with one rule because it dynamically changes meaning according to viewpoints and times. Natural language is difficult to analyze because it can be interpreted differently depending on the meaning or context even if the preprocessing process is well done. Sometimes, analysis considering emoticons( encoding of emotional expression ) is necessary, and recently, emoji symbols are also actively used as an extension concept of natural language\n",
    "\n",
    "**◆ corpus**\n",
    "natural language A collection of related documents contained in the \n",
    "\n",
    "> corpus > paragraph > syntax or sentence > word\n",
    "> word = syllabus + phonemes + affixes + letters\n",
    "\n",
    "also includes metadata ( information about the document , such as author and date of creation ), so these parts must be distinguished when reading the corpus.\n",
    "\n",
    "**◆ token**\n",
    "\n",
    "The unit of text analysis is called a token , and a token is a string encoded in computer binary numbe rs (bytes) to represent text. A token can be a word or a sentence.\n",
    "\n",
    "**◆ Korean Tokenization**\n",
    "\n",
    "Unlike English, Korean has postpositional particles, so you must understand the concept of morpheme. (Usally, Korean language analysis is more complicated than english)\n",
    "Two morphemes :\n",
    "(1) Independent morphemes : self-reliant as words such as\n",
    "(2) Dependent morphemes : morphemes used in combination with other morphemes, such as\n",
    "tokenization of words rather than word tokenization to obtain tokenization similar to English necessary Part -of-speech tagging needs to check which parts of speech\n",
    "\n",
    "**◆ bag of words model**\n",
    "\n",
    "The word is evaluated how often it occurs with other words in a particular situation. See which words are interlocked and appear at the same time.\n",
    "\n",
    "**stop words** : Stop words are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. There is no single universal list of stop words used by all natural language processing tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list. Therefore, any group of words can be chosen as the stop words for a given purpose. such as 'the','a','and', etc...\n",
    "\n",
    "**◆ (n-gram) analysis**\n",
    "\n",
    "By specifying the number of characters or words to be entered as n instead of 1 according to the sequence, it helps to understand the context.\n",
    "\n",
    "Why we use n-gram? due to understand 'context'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fdd87bbdc94d2d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preprocessing step\n",
    "\n",
    "**◆ Refine**\n",
    "\n",
    "Required before and after tokenization as a denoising step. When HTML is parsed, tag names are rem oved, and stopwords and special characters are removed after tokenization.\n",
    "\n",
    "**◆ Tokenization**\n",
    "\n",
    "Tokenize words or sentences by breaking text into desired unit.\n",
    "\n",
    "**◆ Normalization**\n",
    "\n",
    "A work that unifies words written in different forms into standard words. Normalization methods incl ude stem extraction and lemma extraction.\n",
    "\n",
    "**◆ Part-of-speech tagging**\n",
    "\n",
    "Tokenized words are marked by classifying parts of speech, and the same word can have different contexts depending on parts of speech, so it is necessary to distinguish the.\n",
    "\n",
    "#### Regular Expression\n",
    "\n",
    "regular expression is regular expression Or an expression that uses a type of formal language grammar called a regular grammar.\n",
    "Regular expressions are also called lock language. The lock judges whether a given lock language sentence matches a particularly meaningful sentence and presents an appropriate response."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25c94400bfb5e070"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ The re module**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f52cfd46304bcfb2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-18T07:50:12.249864Z",
     "start_time": "2023-09-18T07:50:12.241258Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile('[a-z]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function `re.compile()` Compile the regular expression using , and work with the compiled object p."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d90f656c60df38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(1) String search using regular expression \n",
    "\n",
    "- `match()`: Matches the regular expression\n",
    "- `search()`: Searches\n",
    "- `findall()`: returns all strings (substrings) that match the regular expression return as list \n",
    "- `finditer()`: returns an\n",
    "\n",
    "(2) method of match object \n",
    "\n",
    "- `group()` : returns\n",
    "- `start()` : of the matched string return \n",
    "- `end()`: returns\n",
    "- `span()`: returns\n",
    "\n",
    "(3) Compilation option : used as \n",
    "- `re.DOTALL` or `re.S` : allows\n",
    "- `re.IGNORE` or `re.I` : allows\n",
    "- `re.MULTILINE` or `re.M` : allows matching against\n",
    "- `re.VERBOSE` or `re.X` : to make the regex easier to see and to use comments etc.\n",
    "\n",
    "(4) grouping\n",
    "- `group(0)` : the entire matched string\n",
    "- `group(n)` : String corresponding to the nth group"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f111bedd6217525"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ Meta character**\n",
    "\n",
    "A character used for a separate purpose other than the original meaning of the character\n",
    "\n",
    "\\.\\^\\$\\*\\+\\?\\{\\}\\[\\]\\\\\\|\\(\\) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faed3f9b69185e17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(1) character class [ ], \\  <br>\n",
    "\n",
    "in [ ] Matches characters . However , using ^ in [ ] means not .\n",
    "special When using letters of meaning , uppercase letters mean the opposite of lowercase letters.\n",
    "\n",
    "(2) dot(.)\n",
    "\n",
    "It matches with all letters except for the newline character \\n\n",
    "Ex) a.b : between a and b all characters Note) a[.]b : dot(.) between a and b\n",
    "\n",
    "(3) Repetition (*): The character preceding \\* is repeated \n",
    "\n",
    "Ex) ca*t : ct , cat , caat all applicable\n",
    "\n",
    "(4) Repeat (+): Matches the character before the + repeated\n",
    "\n",
    "(5) repeat ({ m,n }, ? ):\n",
    "\n",
    "{m} : immediately Matches the previous character m times Ex ) do{2}g : doog\n",
    "\n",
    "{m, n} : directly Matching the preceding character from number m to number n is permitted\n",
    "- Ex) do{2,3}g : doog , dooog \n",
    "\n",
    "? : Same meaning as {0,1}\n",
    "\n",
    "(6) Backslash ( \\ ) : Backslash Escape by using twice of backslash\n",
    "\n",
    "Ex) \\ \\section : Search for a string called section. If \\ section is used , it is recognized as \\s of the character class.\n",
    "\n",
    "(7) or ( | ): Ex) a|b : a or b\n",
    "\n",
    "(8) Matching at the beginning of a string ( ^ ): same as\n",
    "\n",
    "Ex) ^My : My to string Matches only if it is at the beginning\n",
    "\n",
    "(9) Matches end of string ( $ ) : Same as\n",
    "\n",
    "Ex) strong$: matches only when strong is located at the end of the string\n",
    "\n",
    "(10) Grouping ( ): Create a group. \n",
    "\n",
    "Ex) ( \\w+) \\ s \\d+ : extracts only"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86f8186f1d5a7556"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77ce524a2c8092f7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('webtext') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T08:43:53.068827Z",
     "start_time": "2023-09-18T08:43:46.269239Z"
    }
   },
   "id": "9eeaa19f09ca9919"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a12da6a3983099f6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
