{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "- Tokens and Normalization Techniques \n",
    "- JSON, CSV, TXT handling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b940ec1336ef55a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Natural Language Processing Overview\n",
    "\n",
    "**◆ The difference between structured and unstructured data**\n",
    " \n",
    "Structured data consists of subjects, variables, and observations, and they are written in a structure (spreadsheet) suitable for general statistical analysis. Unstructured data are documents, images, audio, video, etc., and it is difficult to apply traditiona statistical analysis techniques, and pre-processing is required for analysis.\n",
    "\n",
    "\n",
    "**◆ Difficulty of natural language (text) processing**\n",
    "\n",
    "Text, a representative example of unstructured data, is composed of natural language, and text corresponds to data encoded in natural language. Since natural language has been created and changed with various usages, it has characteristics such as ambiguity, relevance, and ambiguity, and it is difficult to process with one rule because it dynamically changes meaning according to viewpoints and times. Natural language is difficult to analyze because it can be interpreted differently depending on the meaning or context even if the preprocessing process is well done. Sometimes, analysis considering emoticons( encoding of emotional expression ) is necessary, and recently, emoji symbols are also actively used as an extension concept of natural language\n",
    "\n",
    "**◆ corpus**\n",
    "natural language A collection of related documents contained in the \n",
    "\n",
    "> corpus > paragraph > syntax or sentence > word\n",
    "> word = syllabus + phonemes + affixes + letters\n",
    "\n",
    "also includes metadata ( information about the document , such as author and date of creation ), so these parts must be distinguished when reading the corpus.\n",
    "\n",
    "**◆ token**\n",
    "\n",
    "The unit of text analysis is called a token , and a token is a string encoded in computer binary numbe rs (bytes) to represent text. A token can be a word or a sentence.\n",
    "\n",
    "**◆ Korean Tokenization**\n",
    "\n",
    "Unlike English, Korean has postpositional particles, so you must understand the concept of morpheme. (Usally, Korean language analysis is more complicated than english)\n",
    "Two morphemes :\n",
    "(1) Independent morphemes : self-reliant as words such as\n",
    "(2) Dependent morphemes : morphemes used in combination with other morphemes, such as\n",
    "tokenization of words rather than word tokenization to obtain tokenization similar to English necessary Part -of-speech tagging needs to check which parts of speech\n",
    "\n",
    "**◆ bag of words model**\n",
    "\n",
    "The word is evaluated how often it occurs with other words in a particular situation. See which words are interlocked and appear at the same time.\n",
    "\n",
    "**stop words** : Stop words are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. There is no single universal list of stop words used by all natural language processing tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list. Therefore, any group of words can be chosen as the stop words for a given purpose. such as 'the','a','and', etc...\n",
    "\n",
    "**◆ (n-gram) analysis**\n",
    "\n",
    "By specifying the number of characters or words to be entered as n instead of 1 according to the sequence, it helps to understand the context.\n",
    "\n",
    "Why we use n-gram? due to understand 'context'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fdd87bbdc94d2d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preprocessing step\n",
    "\n",
    "**◆ Refine**\n",
    "\n",
    "Required before and after tokenization as a denoising step. When HTML is parsed, tag names are rem oved, and stopwords and special characters are removed after tokenization.\n",
    "\n",
    "**◆ Tokenization**\n",
    "\n",
    "Tokenize words or sentences by breaking text into desired unit.\n",
    "\n",
    "**◆ Normalization**\n",
    "\n",
    "A work that unifies words written in different forms into standard words. Normalization methods incl ude stem extraction and lemma extraction.\n",
    "\n",
    "**◆ Part-of-speech tagging**\n",
    "\n",
    "Tokenized words are marked by classifying parts of speech, and the same word can have different contexts depending on parts of speech, so it is necessary to distinguish the.\n",
    "\n",
    "**◆ Regular expression**\n",
    "\n",
    "regular expression is regular expression Or an expression that uses a type of formal language grammar called a regular grammar.\n",
    "Regular expressions are also called lock language. The lock judges whether a given lock language sentence matches a particularly meaningful sentence and presents an appropriate response."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25c94400bfb5e070"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ The re module**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f52cfd46304bcfb2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-18T07:50:12.249864Z",
     "start_time": "2023-09-18T07:50:12.241258Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile('[a-z]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function `re.compile()` Compile the regular expression using , and work with the compiled object p."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d90f656c60df38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(1) String search using regular expression \n",
    "\n",
    "- `match()`: Matches the regular expression\n",
    "- `search()`: Searches\n",
    "- `findall()`: returns all strings (substrings) that match the regular expression return as list \n",
    "- `finditer()`: returns an"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f111bedd6217525"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9eeaa19f09ca9919"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
