{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "- Tokens and Normalization Techniques \n",
    "- JSON, CSV, TXT handling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b940ec1336ef55a3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Natural Language Processing Overview\n",
    "\n",
    "**◆ The difference between structured and unstructured data**\n",
    " \n",
    "Structured data consists of subjects, variables, and observations, and they are written in a structure (spreadsheet) suitable for general statistical analysis. Unstructured data are documents, images, audio, video, etc., and it is difficult to apply traditiona statistical analysis techniques, and pre-processing is required for analysis.\n",
    "\n",
    "\n",
    "**◆ Difficulty of natural language (text) processing**\n",
    "\n",
    "Text, a representative example of unstructured data, is composed of natural language, and text corresponds to data encoded in natural language. Since natural language has been created and changed with various usages, it has characteristics such as ambiguity, relevance, and ambiguity, and it is difficult to process with one rule because it dynamically changes meaning according to viewpoints and times. Natural language is difficult to analyze because it can be interpreted differently depending on the meaning or context even if the preprocessing process is well done. Sometimes, analysis considering emoticons( encoding of emotional expression ) is necessary, and recently, emoji symbols are also actively used as an extension concept of natural language\n",
    "\n",
    "**◆ corpus**\n",
    "natural language A collection of related documents contained in the \n",
    "\n",
    "> corpus > paragraph > syntax or sentence > word\n",
    "> word = syllabus + phonemes + affixes + letters\n",
    "\n",
    "also includes metadata ( information about the document , such as author and date of creation ), so these parts must be distinguished when reading the corpus.\n",
    "\n",
    "**◆ token**\n",
    "\n",
    "The unit of text analysis is called a token , and a token is a string encoded in computer binary numbe rs (bytes) to represent text. A token can be a word or a sentence.\n",
    "\n",
    "**◆ Korean Tokenization**\n",
    "\n",
    "Unlike English, Korean has postpositional particles, so you must understand the concept of morpheme. (Usally, Korean language analysis is more complicated than english)\n",
    "Two morphemes :\n",
    "(1) Independent morphemes : self-reliant as words such as\n",
    "(2) Dependent morphemes : morphemes used in combination with other morphemes, such as\n",
    "tokenization of words rather than word tokenization to obtain tokenization similar to English necessary Part -of-speech tagging needs to check which parts of speech\n",
    "\n",
    "**◆ bag of words model**\n",
    "\n",
    "The word is evaluated how often it occurs with other words in a particular situation. See which words are interlocked and appear at the same time.\n",
    "\n",
    "**stop words** : Stop words are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. There is no single universal list of stop words used by all natural language processing tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list. Therefore, any group of words can be chosen as the stop words for a given purpose. such as 'the','a','and', etc...\n",
    "\n",
    "**◆ (n-gram) analysis**\n",
    "\n",
    "By specifying the number of characters or words to be entered as n instead of 1 according to the sequence, it helps to understand the context.\n",
    "\n",
    "Why we use n-gram? due to understand 'context'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fdd87bbdc94d2d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Preprocessing step\n",
    "\n",
    "**◆ Refine**\n",
    "\n",
    "Required before and after tokenization as a denoising step. When HTML is parsed, tag names are rem oved, and stopwords and special characters are removed after tokenization.\n",
    "\n",
    "**◆ Tokenization**\n",
    "\n",
    "Tokenize words or sentences by breaking text into desired unit.\n",
    "\n",
    "**◆ Normalization**\n",
    "\n",
    "A work that unifies words written in different forms into standard words. Normalization methods incl ude stem extraction and lemma extraction.\n",
    "\n",
    "**◆ Part-of-speech tagging**\n",
    "\n",
    "Tokenized words are marked by classifying parts of speech, and the same word can have different contexts depending on parts of speech, so it is necessary to distinguish the.\n",
    "\n",
    "#### Regular Expression\n",
    "\n",
    "regular expression is regular expression Or an expression that uses a type of formal language grammar called a regular grammar.\n",
    "Regular expressions are also called lock language. The lock judges whether a given lock language sentence matches a particularly meaningful sentence and presents an appropriate response."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25c94400bfb5e070"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ The re module**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f52cfd46304bcfb2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-18T07:50:12.249864Z",
     "start_time": "2023-09-18T07:50:12.241258Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "p = re.compile('[a-z]+')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function `re.compile()` Compile the regular expression using , and work with the compiled object p."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d90f656c60df38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(1) String search using regular expression \n",
    "\n",
    "- `match()`: Matches the regular expression\n",
    "- `search()`: Searches\n",
    "- `findall()`: returns all strings (substrings) that match the regular expression return as list \n",
    "- `finditer()`: returns an\n",
    "\n",
    "(2) method of match object \n",
    "\n",
    "- `group()` : returns\n",
    "- `start()` : of the matched string return \n",
    "- `end()`: returns\n",
    "- `span()`: returns\n",
    "\n",
    "(3) Compilation option : used as \n",
    "- `re.DOTALL` or `re.S` : allows\n",
    "- `re.IGNORE` or `re.I` : allows\n",
    "- `re.MULTILINE` or `re.M` : allows matching against\n",
    "- `re.VERBOSE` or `re.X` : to make the regex easier to see and to use comments etc.\n",
    "\n",
    "(4) grouping\n",
    "- `group(0)` : the entire matched string\n",
    "- `group(n)` : String corresponding to the nth group"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f111bedd6217525"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ Meta character**\n",
    "\n",
    "A character used for a separate purpose other than the original meaning of the character\n",
    "\n",
    "\\.\\^\\$\\*\\+\\?\\{\\}\\[\\]\\\\\\|\\(\\) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faed3f9b69185e17"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(1) character class [ ], \\  <br>\n",
    "\n",
    "in [ ] Matches characters . However , using ^ in [ ] means not .\n",
    "special When using letters of meaning , uppercase letters mean the opposite of lowercase letters.\n",
    "\n",
    "(2) dot(.)\n",
    "\n",
    "It matches with all letters except for the newline character \\n\n",
    "Ex) a.b : between a and b all characters Note) a[.]b : dot(.) between a and b\n",
    "\n",
    "(3) Repetition (*): The character preceding \\* is repeated \n",
    "\n",
    "Ex) ca*t : ct , cat , caat all applicable\n",
    "\n",
    "(4) Repeat (+): Matches the character before the + repeated\n",
    "\n",
    "(5) repeat ({ m,n }, ? ):\n",
    "\n",
    "{m} : immediately Matches the previous character m times Ex ) do{2}g : doog\n",
    "\n",
    "{m, n} : directly Matching the preceding character from number m to number n is permitted\n",
    "- Ex) do{2,3}g : doog , dooog \n",
    "\n",
    "? : Same meaning as {0,1}\n",
    "\n",
    "(6) Backslash ( \\ ) : Backslash Escape by using twice of backslash\n",
    "\n",
    "Ex) \\ \\section : Search for a string called section. If \\ section is used , it is recognized as \\s of the character class.\n",
    "\n",
    "(7) or ( | ): Ex) a|b : a or b\n",
    "\n",
    "(8) Matching at the beginning of a string ( ^ ): same as\n",
    "\n",
    "Ex) ^My : My to string Matches only if it is at the beginning\n",
    "\n",
    "(9) Matches end of string ( $ ) : Same as\n",
    "\n",
    "Ex) strong$: matches only when strong is located at the end of the string\n",
    "\n",
    "(10) Grouping ( ): Create a group. \n",
    "\n",
    "Ex) ( \\w+) \\ s \\d+ : extracts only"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86f8186f1d5a7556"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77ce524a2c8092f7"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package webtext to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/webtext.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') \n",
    "nltk.download('webtext') \n",
    "nltk.download('wordnet') \n",
    "nltk.download('stopwords') \n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('omw-1.4')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T08:43:53.068827Z",
     "start_time": "2023-09-18T08:43:46.269239Z"
    }
   },
   "id": "9eeaa19f09ca9919"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. toekn split**\n",
    "\n",
    "(1) one Split a sentence (split(): Convert a string to a list )\n",
    "\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of twenty-six.\"\"\" \n",
    "\n",
    "`sentence.split ()`\n",
    "(Three quotes are just newlines Line wrapping output possible : Try using `print()`)\n",
    "\n",
    "(2) Can be extended to a corpus composed of one or more sentences : Paragraphs are divided into sentences , and each s entence is tokenized. corpus = [\"Thomas Jefferson began building Monticello at the age of twenty-six.\",\n",
    "\"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "\"Wondering, she opened the door to the studio.\" ] corpus_split = [x.split() for x in corpus]\n",
    "\n",
    "- tokenize.sent_tokenize(p) 문장으로 분할 \n",
    "- tokenize.word_tokenize(p) 단어로 분할 "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "192b152832a0c1b1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "p = \"She is a heroine. He is an AI developer. Don't be late, Mr. Kim's house is: very-near here!\"\n",
    "tokenize.sent_tokenize(p) # Ctrl+Shif+Enter\n",
    "tokenize.word_tokenize(p) # Don't과 Kim's ?\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text = '''She is a heroine. He is an AI developer. Don't be late, Mr. Kim's house is: very-near here!'''\n",
    "tokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:18:47.614269Z",
     "start_time": "2023-09-20T08:18:47.121668Z"
    }
   },
   "id": "a12da6a3983099f6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She', 'is', 'a', 'heroine', '.', 'He', 'is', 'an', 'AI', 'developer', '.', 'Do', \"n't\", 'be', 'late', ',', 'Mr.', 'Kim', \"'s\", 'house', 'is', ':', 'very-near', 'here', '!']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:18:58.530589Z",
     "start_time": "2023-09-20T08:18:58.521795Z"
    }
   },
   "id": "5e4c2042a91266c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Token Frequency analysis**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfd20d70b673651c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Counter({'Good': 1, 'morning,': 1, 'Kim': 1})"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "corpus_split = Counter(\"Good morning, Kim\".split()) \n",
    "#Counter(\"Good morning, Kim!\".split()) \n",
    "Counter(corpus_split) # use"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:20:08.846119Z",
     "start_time": "2023-09-20T08:20:08.838153Z"
    }
   },
   "id": "a6990cd51d2ab9b6"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'is': 3, '.': 2, 'She': 1, 'a': 1, 'heroine': 1, 'He': 1, 'an': 1, 'AI': 1, 'developer': 1, 'Do': 1, \"n't\": 1, 'be': 1, 'late': 1, ',': 1, 'Mr.': 1, 'Kim': 1, \"'s\": 1, 'house': 1, ':': 1, 'very-near': 1, 'here': 1, '!': 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(tokens)) # 반환은 사전 형태이다. 불용어도 없어지면 더 사용하기 좋을 듯 하다"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:20:33.565500Z",
     "start_time": "2023-09-20T08:20:33.557334Z"
    }
   },
   "id": "e6966c64dd71522e"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "10"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# of words combination : Modification of order (translation of meaning is possible in relation to grammar)\n",
    "from itertools import permutations\n",
    "[\" \".join(combo) for combo in permutations('Good morning Rosa!'.split(), 3)]\n",
    "s = \"Find textbooks with titles containing 'NLP', or 'natural' and 'language'.\"\n",
    "len(set(s.split()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:21:22.924408Z",
     "start_time": "2023-09-20T08:21:22.918027Z"
    }
   },
   "id": "d5eeeb1e11fd52f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(Note) \" \". join(A): Convert A (consisting of multiple elements) in list form into a string in the form of jumping (each element)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77e5092f7a44ff1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Regular Expressions**\n",
    "\n",
    "(1) String search using regular expression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b79851ca9c4efe0"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile('[az]+') # character repeated at least 1 time\n",
    "\n",
    "m = p.match('python') # match: If the first value of the string matches, return the result \n",
    "print(m)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:30:57.569823Z",
     "start_time": "2023-09-20T08:30:57.562145Z"
    }
   },
   "id": "8a4add2fc29f5a1e"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroup\u001B[49m() \u001B[38;5;66;03m# group() : return match string\u001B[39;00m\n\u001B[1;32m      2\u001B[0m m\u001B[38;5;241m.\u001B[39mstart() \u001B[38;5;66;03m# start() : start position of match string\u001B[39;00m\n\u001B[1;32m      3\u001B[0m m\u001B[38;5;241m.\u001B[39mend() \u001B[38;5;66;03m# end(): end position of match string\u001B[39;00m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "m.group() # group() : return match string\n",
    "m.start() # start() : start position of match string\n",
    "m.end() # end(): end position of match string\n",
    "m.span() # span(): ( start , end ) position of match string\n",
    "\n",
    "m1 = p.match(\"3 python\") # match: because the first value of the string is a number None output \n",
    "print(m1)\n",
    "\n",
    "p.search(\"python\").group() # search: find matching value in entire string and return if found\n",
    "\n",
    "print(p. findall(\"life is too short\")) # findall : returns a list of all strings matching the regular expression p.findall(\"life is too short\").group() # ERROR : cannot use group in list\n",
    "print(p. finditer (\"life is too short\")) # finditer : similar to findall , but returns an object"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:31:08.165123Z",
     "start_time": "2023-09-20T08:31:08.145139Z"
    }
   },
   "id": "bf61cc330cdd193"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(2) compile option"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5455062b56eb7317"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python one']\n"
     ]
    }
   ],
   "source": [
    "p = re.compile('a.b', re.DOTALL) \n",
    "p.match('a\\nb').group()\n",
    "p = re.compile('[a-z]', re.I) \n",
    "p.match('PyThon').group() \n",
    "p.match('pYTHON').group()\n",
    "#^python\\s\\w+ must ( on each line ) start with the string python, followed by whitespace, followed by a word \n",
    "\n",
    "p = re.compile(\"^python\\s\\w+\", re.MULTILINE)\n",
    "data = \"\"\"python one life is too short python two you need python python three\"\"\"\n",
    "print(p.findall(data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:32:31.366056Z",
     "start_time": "2023-09-20T08:32:31.353677Z"
    }
   },
   "id": "df37b63208fee5af"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python one', 'python two', 'python three']\n"
     ]
    }
   ],
   "source": [
    "data = \"\"\"python one \n",
    "life is too short \n",
    "python two \n",
    "you need python \n",
    "python three\"\"\" \n",
    "print(p.findall(data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:33:02.491480Z",
     "start_time": "2023-09-20T08:33:02.483891Z"
    }
   },
   "id": "a047898b841699b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(3) compile and string search together"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9cb2d6a06232dbb"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile('Crow|Servo')\n",
    "p.match('CrowHello').group()\n",
    "re.search('^Life', 'Life is too short').group()\n",
    "re.search('short$', 'Life is too short').group()\n",
    "print( re.search ('short$', 'Life is too short, you need python'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:34:25.804735Z",
     "start_time": "2023-09-20T08:34:25.793366Z"
    }
   },
   "id": "1502d3614b7c814e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(4) grouping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "692f29a17dc68554"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "'ABCABCABC'"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile(\"(ABC)+\")\n",
    "p.search('ABCABCABC OK?').group()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:36:02.618862Z",
     "start_time": "2023-09-20T08:36:02.596450Z"
    }
   },
   "id": "8248c33d8934fb5f"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "'park 010-1234-1234'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r\"\\w+\\s+\\d+[-]\\d+[-]\\d+\")\n",
    "p.search(\"park 010-1234-1234\").group()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:36:19.037373Z",
     "start_time": "2023-09-20T08:36:19.013102Z"
    }
   },
   "id": "f6f1e5f767bd70da"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "'park'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(r\"(\\w+)\\s+(\\d+[-]\\d+[-]\\d+)\") \n",
    "p.search(\"park 010-1234-1234\").group(1) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:36:55.635273Z",
     "start_time": "2023-09-20T08:36:55.619692Z"
    }
   },
   "id": "1f398679e9a67b78"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "'010-1234-1234'"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.search(\"park 010-1234-1234\").group(2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:36:57.306344Z",
     "start_time": "2023-09-20T08:36:57.283857Z"
    }
   },
   "id": "f30ef2467fe4d811"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(5) Forward searching"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd9432e87b1c0d3e"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "'http:'"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile(\".+:\")\n",
    "p.search(\"http://google.com\").group()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:37:49.046810Z",
     "start_time": "2023-09-20T08:37:49.020432Z"
    }
   },
   "id": "4e22b59c03377d0"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "'http'"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = re.compile(\".+(?=:)\")\n",
    "p.search(\"http://google.com\").group()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:38:23.849069Z",
     "start_time": "2023-09-20T08:38:23.823768Z"
    }
   },
   "id": "7b7ff29aa26ab575"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(6) String replacement"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc06a72f72dca11a"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "('colour sucks and colour shoes', 2)"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "p = re.compile('(blue|white|red)')\n",
    "p.subn('colour','blue sucks and red shoes')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:39:49.307802Z",
     "start_time": "2023-09-20T08:39:49.280414Z"
    }
   },
   "id": "6fcc3b4be783f5b2"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 32)\n"
     ]
    }
   ],
   "source": [
    "s = '<html><head><title>Title</title>'\n",
    "print(re.match('<.*>', s).span())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:40:11.867891Z",
     "start_time": "2023-09-20T08:40:11.838868Z"
    }
   },
   "id": "7ee7a22b5107fef3"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><title>Title</title>\n"
     ]
    }
   ],
   "source": [
    "print(re.match('<.*>', s).group())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:40:25.792328Z",
     "start_time": "2023-09-20T08:40:25.770506Z"
    }
   },
   "id": "c065ca994a9653b4"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n"
     ]
    }
   ],
   "source": [
    "print(re.match('<.*?>', s).group())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:40:31.154983Z",
     "start_time": "2023-09-20T08:40:31.134359Z"
    }
   },
   "id": "65171bb8fccc0fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "(7) HTML tag remove"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c74853a55c127a09"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "import re\n",
    "html = (\"<html><head>some header information</head> \\\n",
    "<Body>it's start. <script src='..'>some script</script> \\\n",
    "<!-- some comments -->some <b>body</b> contents.. \\n <a href ='some link'> gogo </a> \\\n",
    "and other stuff .. <script>another</script></Body></html>\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:23.576447Z",
     "start_time": "2023-09-20T08:43:23.560227Z"
    }
   },
   "id": "8ed3529096407a9a"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "str"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(html)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:23.900498Z",
     "start_time": "2023-09-20T08:43:23.886359Z"
    }
   },
   "id": "7c31f0900f0c133"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "240"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(html)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:24.208840Z",
     "start_time": "2023-09-20T08:43:24.203495Z"
    }
   },
   "id": "3047fde9a0d47082"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "\"<Body>it's start. <script src='..'>some script</script> <!-- some comments -->some <b>body</b> contents.. \\n <a href ='some link'> gogo </a> and other stuff .. <script>another</script></Body>\""
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body = re.search('<body.*/body>', html, re.I|re.S).group()\n",
    "body"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:24.506902Z",
     "start_time": "2023-09-20T08:43:24.502673Z"
    }
   },
   "id": "535b62ac92fb37bf"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "data": {
      "text/plain": "\"<Body>it's start.  <!-- some comments -->some <b>body</b> contents.. \\n <a href ='some link'> gogo </a> and other stuff .. </Body>\""
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('<script.*?>.*?</script>', '', body, 0, re.I|re.S)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:24.951252Z",
     "start_time": "2023-09-20T08:43:24.948218Z"
    }
   },
   "id": "1456505b639a031e"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's start. some script some body contents.. \n",
      "  gogo  and other stuff .. another\n"
     ]
    }
   ],
   "source": [
    "text = re.sub('<.+?>', '', body, 0, re.I|re.S) \n",
    "print(text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:25.301525Z",
     "start_time": "2023-09-20T08:43:25.298121Z"
    }
   },
   "id": "ff4fbd7d5c2fc08d"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's start some script some body contents   gogo  and other stuff  another\n"
     ]
    }
   ],
   "source": [
    "result = re.sub('\\t|\\r|\\n|\\.', '', text) \n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-20T08:43:26.013491Z",
     "start_time": "2023-09-20T08:43:26.008756Z"
    }
   },
   "id": "58412923cc7f3b16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9. Examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c48c25691d4ddf63"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "raw_review = '''One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutal ity and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentar y. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christia ns, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /> <br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forge t pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first e pisode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I develo ped a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class i nmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.'''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:31:02.218887Z",
     "start_time": "2023-09-25T08:31:01.417706Z"
    }
   },
   "id": "668cfbe01e835a03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove HTML tag first"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d67471ff8107946"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "review_text = BeautifulSoup(raw_review, 'html.parser').get_text()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:31:06.511442Z",
     "start_time": "2023-09-25T08:31:06.476109Z"
    }
   },
   "id": "d91c635bed978fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert non-alphabetic characters to spaces"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1724dfac8a2a79f3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "letters_only = re.sub('[^a-zA-Z]', ' ', review_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:31:25.009461Z",
     "start_time": "2023-09-25T08:31:24.998845Z"
    }
   },
   "id": "5ac8a2f879ea2465"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert lower case"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a0cc87a78cbcd91"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "words = letters_only.lower().split()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:31:36.984886Z",
     "start_time": "2023-09-25T08:31:36.959917Z"
    }
   },
   "id": "8f34d8b1d1b0ddf3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert stopwords to sets. It's much faster to search in a set than in a list in Python"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d63e7d9b8cf531eb"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:34:20.770823Z",
     "start_time": "2023-09-25T08:34:20.757294Z"
    }
   },
   "id": "2324b577c5a6b44c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "stopwords eliminate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e49f62448d212c86"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "meaningfil_words = [word for word in words if not word in stops]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:34:53.079612Z",
     "start_time": "2023-09-25T08:34:53.069882Z"
    }
   },
   "id": "49af060349d8ce1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "stemming"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "929ecc0105e54e82"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "stemming_words = [ps.stem(word) for word in meaningfil_words]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:36:57.106799Z",
     "start_time": "2023-09-25T08:36:57.076946Z"
    }
   },
   "id": "5463d9628fca814a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combine into space seperated strings and return the result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a144af9628f13fe"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one review mention watch oz episod hook right exactli happen first thing struck oz brutal iti unflinch scene violenc set right word go trust show faint heart timid show pull punch regard drug sex violenc hardcor classic use word call oz nicknam given oswald maximum secur state penitentar focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home mani aryan muslim gangsta latino christia ns italian irish scuffl death stare dodgi deal shadi agreement never far away would say main appeal show due fact goe show dare forg pretti pictur paint mainstream audienc forget charm forget romanc oz mess around first e pisod ever saw struck nasti surreal say readi watch develo ped tast oz got accustom high level graphic violenc violenc injustic crook guard sold nickel inmat kill order get away well manner middl class nmate turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort view that get touch darker side\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(stemming_words))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:37:28.200200Z",
     "start_time": "2023-09-25T08:37:28.170129Z"
    }
   },
   "id": "b3adb52c1a6e2804"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 10. Korean Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "806019053a40f280"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import kss\n",
    "text = \"4차 산업 혁명이라는 용어가 나온 이후로 우리의 산업화의 방향은 하루가 다르게 변화하고 있습니다. 그 가운데에 [AI: 인공지능]과 [빅데이터]라는 화두는 사회 곳곳에 빠짐없이 등장하고 있습니다. 교육계에서도 이러한 사회 경제적 수 요를 충족시키기 위해서 경쟁적으로 AI 빅데이터 대학원 및 단기 교육과정들이 전국적으로 우후죽순으로 설립되고 있습니 다. 그러나, AI 빅데이터 기술을 어떻게 비즈니스 영역에 연결할 수 있는지에 대한 프로그램은 아직까지는 전무하다고 할 수 있습니다. 아무리 세계적 수준의 우수한 기술을 습득한다고 할지라도 이를 사업화 할 수 있는 역량이 부족하다면 성공으로 나아갈 수 있는 중요한 관문의 열쇠가 없는 것과 같은 상황일 것입니다.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:38:29.945579Z",
     "start_time": "2023-09-25T08:38:29.778862Z"
    }
   },
   "id": "93952b421e143f43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "split_sentences함수는 문장을 나누는 역할을 하는 함수이다"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59ec0f2a755456ae"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://github.com/hyunwoongko/python-mecab-kor\n",
      "- konlpy.tag.Mecab: https://konlpy.org/en/latest/api/konlpy.tag/#mecab-class\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4차 산업 혁명이라는 용어가 나온 이후로 우리의 산업화의 방향은 하루가 다르게 변화하고 있습니다.', '그 가운데에 [AI: 인공지능]과 [빅데이터]라는 화두는 사회 곳곳에 빠짐없이 등장하고 있습니다.', '교육계에서도 이러한 사회 경제적 수 요를 충족시키기 위해서 경쟁적으로 AI 빅데이터 대학원 및 단기 교육과정들이 전국적으로 우후죽순으로 설립되고 있습니 다.', '그러나, AI 빅데이터 기술을 어떻게 비즈니스 영역에 연결할 수 있는지에 대한 프로그램은 아직까지는 전무하다고 할 수 있습니다.', '아무리 세계적 수준의 우수한 기술을 습득한다고 할지라도 이를 사업화 할 수 있는 역량이 부족하다면 성공으로 나아갈 수 있는 중요한 관문의 열쇠가 없는 것과 같은 상황일 것입니다.']\n"
     ]
    }
   ],
   "source": [
    "print(kss.split_sentences(text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:38:41.960614Z",
     "start_time": "2023-09-25T08:38:41.390704Z"
    }
   },
   "id": "3d9c704904e00c4"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "JVMNotFoundException",
     "evalue": "No JVM shared library file (libjli.dylib) found. Try setting up the JAVA_HOME environment variable properly.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mJVMNotFoundException\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkonlpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Okt\n\u001B[0;32m----> 2\u001B[0m okt \u001B[38;5;241m=\u001B[39m \u001B[43mOkt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAI Big Data Innovation MBA trains AI big data experts with commercialization capabilities.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/Webscraping/lib/python3.10/site-packages/konlpy/tag/_okt.py:51\u001B[0m, in \u001B[0;36mOkt.__init__\u001B[0;34m(self, jvmpath, max_heap_size)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, jvmpath\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, max_heap_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m):\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m jpype\u001B[38;5;241m.\u001B[39misJVMStarted():\n\u001B[0;32m---> 51\u001B[0m         \u001B[43mjvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_jvm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mjvmpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_heap_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m     oktJavaPackage \u001B[38;5;241m=\u001B[39m jpype\u001B[38;5;241m.\u001B[39mJPackage(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkr.lucypark.okt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     54\u001B[0m     OktInterfaceJavaClass \u001B[38;5;241m=\u001B[39m oktJavaPackage\u001B[38;5;241m.\u001B[39mOktInterface\n",
      "File \u001B[0;32m~/anaconda3/envs/Webscraping/lib/python3.10/site-packages/konlpy/jvm.py:55\u001B[0m, in \u001B[0;36minit_jvm\u001B[0;34m(jvmpath, max_heap_size)\u001B[0m\n\u001B[1;32m     52\u001B[0m args \u001B[38;5;241m=\u001B[39m [javadir, os\u001B[38;5;241m.\u001B[39msep]\n\u001B[1;32m     53\u001B[0m classpath \u001B[38;5;241m=\u001B[39m [f\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;241m*\u001B[39margs) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m folder_suffix]\n\u001B[0;32m---> 55\u001B[0m jvmpath \u001B[38;5;241m=\u001B[39m jvmpath \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mjpype\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetDefaultJVMPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# NOTE: Temporary patch for Issue #76. Erase when possible.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sys\u001B[38;5;241m.\u001B[39mplatform \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdarwin\u001B[39m\u001B[38;5;124m'\u001B[39m\\\n\u001B[1;32m     59\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m jvmpath\u001B[38;5;241m.\u001B[39mfind(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1.8.0\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\\\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m jvmpath\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlibjvm.dylib\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "File \u001B[0;32m~/anaconda3/envs/Webscraping/lib/python3.10/site-packages/jpype/_jvmfinder.py:74\u001B[0m, in \u001B[0;36mgetDefaultJVMPath\u001B[0;34m()\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     73\u001B[0m     finder \u001B[38;5;241m=\u001B[39m LinuxJVMFinder()\n\u001B[0;32m---> 74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfinder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_jvm_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/Webscraping/lib/python3.10/site-packages/jpype/_jvmfinder.py:212\u001B[0m, in \u001B[0;36mJVMFinder.get_jvm_path\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m jvm_notsupport_ext \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    211\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m jvm_notsupport_ext\n\u001B[0;32m--> 212\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m JVMNotFoundException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo JVM shared library file (\u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m) \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    213\u001B[0m                            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound. Try setting up the JAVA_HOME \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    214\u001B[0m                            \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menvironment variable properly.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    215\u001B[0m                            \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_libfile))\n",
      "\u001B[0;31mJVMNotFoundException\u001B[0m: No JVM shared library file (libjli.dylib) found. Try setting up the JAVA_HOME environment variable properly."
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "text = \"AI Big Data Innovation MBA trains AI big data experts with commercialization capabilities.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-25T08:44:11.958559Z",
     "start_time": "2023-09-25T08:44:11.841830Z"
    }
   },
   "id": "3a35cf193de5fafc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8faf03fcd6fc62fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
