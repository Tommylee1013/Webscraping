{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Text Data Structure\n",
    "\n",
    "#### Word Expreassion\n",
    "\n",
    "**◆ word vectors (word representations)**\n",
    "- The most basic problem of natural language processing is how to make a computer recognize natural language.\n",
    "- Computer recognizes natural language as binary code ( Unicode , ASCII code ,…). \n",
    "    Ex) English : 1100010110111000, English : 1100010110110100\n",
    "- This way of expression has no characteristics of words at all.\n",
    "- It can be used for classification and clustering.\n",
    "\n",
    "**◆ One-Hot encoding (one-hot encoding)**\n",
    "\n",
    "words It is expressed as a single vector , with only one 1 at a specific position, and the rest are marked as 0. Ex) {Thomas Jepperson made Jepperson building}\n",
    "\n",
    "<center>\n",
    "\n",
    "||Thomas|Jepperson|made|building|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|Thomas|1|0|0|0|\n",
    "|Japperson|0|1|0|0|\n",
    "|made|0|0|1|0|\n",
    "|Japperson|0|1|0|0|\n",
    "|building|0|0|0|1|\n",
    "\n",
    "</center>\n",
    "\n",
    "- To know what the nth word is in a row ( sentence ) , you need to know the column value that is 1 in that row. (100% restorable)\n",
    "- One row is each binary row vector, where only one is 1 and the rest are 0.\n",
    "- Columns act as a dictionary of words (terms)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81be61005e2b801"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ One-hot Disadvantages and Alternatives**\n",
    "\n",
    "- one - hot The disadvantage of encoding is that it becomes inefficient because the size of the vector becomes\n",
    "large when there are many words.\n",
    "- To overcome various disadvantages, two alternatives are proposed.\n",
    "\n",
    "**◆ Two alternatives**\n",
    "1. frequency information based\n",
    "\n",
    "    a. word frequency vector (bag of words)\n",
    "   \n",
    "    b. word - document matrix method (TF-IDF , etc. )\n",
    "   \n",
    "    c. co-occurrence matrix : word - word matrix , document - document matrix\n",
    "   \n",
    "2. Meaning ( subject / characteristic ) information-based\n",
    "\n",
    "    a. subject vector ( semantic vector )\n",
    "   \n",
    "    b. word2vec/ Glove ( method is different, but the solution is the same )\n",
    "   \n",
    "    c. BERT, GPT, ...\n",
    "    \n",
    "**◆ Word frequency vector (word collection vector): Bag of words**\n",
    "- A method of trying to understand the meaning of a sentence only with word collection data (word frequency), ignoring the order and grammar of words in a sentence\n",
    "- Unlike the one-hot vector, it contains only the number of appearances, so it is difficult to reproduce the document .\n",
    "- In the most basic way, there is a vector of binary word collections .\n",
    "- Binary word collection vectors are useful for document search indexing, which tells which word is used in which document ."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee1974da398397a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Integer Encoding & Padding\n",
    "\n",
    "**◆ Integer encoding**\n",
    "- It is the basic step among several techniques for converting text to numbers in natural language processing.\n",
    "- A preprocessing task that maps each word to a unique integer.\n",
    "- If there are 5,000 words in the text, unique integers mapped to words from 1 to 5,000 in each of the 5,000 words, In other words, an index is given, usually after sorting by word frequency.\n",
    "- One of the ways to assign integers to words is to create a set of words (vocabulary) in which words are sorted in order of frequency. There is a way to assign integers from lowest to highest in order\n",
    "\n",
    "**◆ padding**\n",
    "- Each sentence ( or document ) can be of different lengths , but the machine divides all documents of the same length into one matrix. Reports can be grouped together and processed .\n",
    "- Arbitrarily equalizing the length of several sentences for parallel operation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9befed24f2c2226"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Word document procession\n",
    "\n",
    "**◆ word Frequency (Term Frequency: TF)**\n",
    "- the number of times the word appeared in the document\n",
    "- If a particular word appears frequently in a particular document, the word is said to be closely related to that document.\n",
    "\n",
    "```\n",
    "// Example\n",
    "\n",
    "Doc1 : the fox chases the rabbit\n",
    "Doc2 : the rabbit ate the cabbage\n",
    "Doc3 : the fox caught the rabbit\n",
    "```\n",
    "\n",
    "Rows are words, columns are documents(TDM: Term-Document Matrix)\n",
    "\n",
    "<center>\n",
    "\n",
    "|          | Doc1 | Doc2 | Doc3 |\n",
    "|:--------:|:----:|:----:|:----:|\n",
    "|   the    |  2   |  2   |  2   |\n",
    "|   fox    |  1   |  0   |  1   |\n",
    "|  rabbit  |  1   |  1   |  1   |\n",
    "|  chases  |  1   |  0   |  0   |\n",
    "|  caught  |  0   |  0   |  1   |\n",
    "| cabbage  |  0   |  1   |  0   |\n",
    "|   ate    |  0   |  1   |  0   |\n",
    "\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34e2f8d014190e42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Word document matrix\n",
    "\n",
    "**◆ word frequency reverse document frequency**\n",
    "- **Zipf's Law** : The frequency of use of any word is inversely proportional to the rank of that word. (Ex: 1st place is 3 times as frequent as 3rd place)\n",
    "- Give low weight to words that appear frequently in the document but do not help to understand the meaning of the document -> IDF\n",
    "\n",
    "\n",
    "**IDF**\n",
    "\n",
    "A weight that measures the importance of a word. \n",
    "\n",
    "$$\\mathrm{IDF} = \\log(\\frac{\\mathrm{N}}{\\mathrm{DF}})$$\n",
    "\n",
    "N is the total number of documents. DF is the document frequency (the number of documents in which the word appears)\n",
    "\n",
    "- The smaller the DF, the higher the importance of the word.\n",
    "- The higher the IDF, the higher the importance of the word\n",
    "- Words with high TF–IDF values give high discrimination in documents (important words in information retrieval)\n",
    "- Calculate TF-IDF : (TF-IDF)(t, d)=TF(t, d) x IDF(t), where t is the word and d is the document\n",
    "\n",
    "<center>\n",
    "\n",
    "|          | Doc1 | Doc2 | Doc3 | DF | N/DF | IDF=$\\log_2(\\mathrm{N}/\\mathrm{DF})$ |\n",
    "|:--------:|:----:|:----:|:----:|:--:|:----:|:------------------------------------:| \n",
    "|   the    |  2   |  2   |  2   | 3  | 3/3  |            $\\log_2(3/3)$             |\n",
    "|   fox    |  1   |  0   |  1   | 2  | 3/2  |            $\\log_2(3/2)$             |\n",
    "|  rabbit  |  1   |  1   |  1   | 3  | 3/3  |            $\\log_2(3/3)$             |\n",
    "|  chases  |  1   |  0   |  0   | 1  | 3/1  |            $\\log_2(3/1)$             |\n",
    "|  caught  |  0   |  0   |  1   | 1  | 3/1  |            $\\log_2(3/1)$             |\n",
    "| cabbage  |  0   |  1   |  0   | 1  | 3/1  |            $\\log_2(3/1)$             |\n",
    "|   ate    |  0   |  1   |  0   | 1  | 3/1  |            $\\log_2(3/1)$             |\n",
    "\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f39116af5182174b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ TF standardization and regularization**\n",
    "- The longer the length of the document, the higher the frequency of occurrence of the word and\n",
    "the higher the possibility of being searched.\n",
    "- Thus the longer the length of the document, the higher the possibility of similarity with other documents .\n",
    "- Standardization and normalization of TF is necessary to complicate these week-points.\n",
    "\n",
    "**◆ Standardization** \n",
    "\n",
    "$$z = \\frac{\\mathrm{TF}-\\mu(\\mathrm{TF})}{\\sigma(\\mathrm{TF})}$$\n",
    "\n",
    "Example) doc 1\n",
    "\n",
    "$$\\mu(\\mathrm{TF}) = \\frac{5}{7} ~~ (\\frac{\\mathrm{number~of~occurrences}}{\\mathrm{total~number~of~words}})$$\n",
    "$$\\sigma(\\mathrm{TF}) = \\sqrt{\\frac{(2 - \\mu)^2 + 3(1 - \\mu)^2 + 3(0-\\mu)^2}{6}}$$\n",
    "\n",
    "<center>\n",
    "\n",
    "|          |   Doc1    |   Doc2    |    Doc3    |\n",
    "|:--------:|:---------:|:---------:|:----------:|\n",
    "|   the    |  1.70084  |  1.70084  |  1.70084   |\n",
    "|   fox    |  0.37796  | -0.944911 |  0.37796   |\n",
    "|  rabbit  |  0.37796  |  0.37796  |  0.37796   |\n",
    "|  chases  |  0.37796  | -0.944911 | -0.944911  |\n",
    "|  caught  | -0.944911 | -0.944911 |  0.37796   |\n",
    "| cabbage  | -0.944911 |  0.37796  | -0.944911  |\n",
    "|   ate    | -0.944911 |  0.37796  | -0.944911  |\n",
    "\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aec89c5d30c31b7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ Normalization**\n",
    "\n",
    "divide TF by the total frequency of the word (1+log(TF))/ ni\n",
    "\n",
    "ni : frequency count of total words in\n",
    "\n",
    "<center>\n",
    "\n",
    "|          |   Doc1    |   Doc2    | Doc3 |\n",
    "|:--------:|:---------:|:---------:|:----:|\n",
    "|   the    |    0.4    |    0.4    | 0.4  |\n",
    "|   fox    |    0.2    |     0     | 0.2  |\n",
    "|  rabbit  |    0.2    |    0.2    | 0.2  |\n",
    "|  chases  |    0.2    |     0     |  0   |\n",
    "|  caught  |     0     |     0     | 0.2  |\n",
    "| cabbage  |     0     |    0.2    |  0   |\n",
    "|   ate    |     0     |    0.2    |  0   |\n",
    "\n",
    "</center>\n",
    "\n",
    "where $0.4 = \\frac{1 + \\log_2{2}}{5}$ and $0.2 = \\frac{1 + \\log_2{1}}{5}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "572953d417ff4d15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ Normalized TF-IDF: Normalized TF times IDF**\n",
    "\n",
    "<center>\n",
    "\n",
    "| Normalized TF-IDF |                Doc1                |             Doc2             |  Doc3   |\n",
    "|:-----------------:|:----------------------------------:|:----------------------------:|:-------:|\n",
    "|        the        |    0 = $0.4 \\times \\log_2(3/3)$    | 0 = $0.4 \\times \\log_2(3/3)$ |    0    |\n",
    "|        fox        | 0.11699 = $0.2 \\times \\log_2(3/2)$ |              0               | 0.11699 |\n",
    "|      rabbit       |    0 = $0.2 \\times \\log_2(3/3)$    |              0               |    0    |\n",
    "|      chases       | 0.31699 = $0.2 \\times \\log_2(3/1)$ |              0               |    0    |\n",
    "|      caught       |                 0                  |              0               | 0.31699 |\n",
    "|      cabbage      |                 0                  |           0.31699            |    0    |\n",
    "|        ate        |                 0                  |           0.31699            |    0    |\n",
    "\n",
    "</center>\n",
    "\n",
    "**◆ Disadvantages of**\n",
    "- Vectors of two pieces of text with different words, even though they have similar meanings (subjects), are in the TF-IDF vector space. If words with the same meaning but different spellings, TF-IDF vectors do not lie close together in the vector space.\n",
    "\n",
    "**The TF-IDF method is difficult to use in the process of finding documents that are similar in meaning (topic)**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7890e1b1f354696c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Co-Occurrence Matrix\n",
    "\n",
    "**◆ joint ( simultaneous ) occurrence matrix**\n",
    "\n",
    "A method of directly counting the number of times words appear simultaneously in a particular context. The number of simultaneous appearances is expressed as a matrix and the matrix is digitized to create word vectors.\n",
    "\n",
    "```\n",
    "Ex) \n",
    "Myeong-seok and Jun-seon went to America\n",
    "Myeong-seok and Sang-ho went to the library\n",
    "Myeong-seok and Jun-seon like cold noodles\n",
    "```\n",
    "\n",
    "< Co-occurrence matrix : word - word matrix > → square matrix , symmetric matrix\n",
    "\n",
    "<center>\n",
    "\n",
    "![wordvector](./images/wordvector.png)\n",
    "\n",
    "</center>\n",
    "\n",
    "→ Can be used as a social network analysis (e.g., calculating the centrality of each word (degree of connection , proximity , median , eigenvector))\n",
    "\n",
    "- TDM : Term based Matrix\n",
    "\n",
    "- DTM : Document based Matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "345dd434cd45d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Word Embedding\n",
    "\n",
    "**◆ Word embedding (word embedding)**\n",
    "- A one-hot vector is a sparse representation with many 0 's and only one 1’.\n",
    "- In contrast to sparse representation , the size of the vector is determined by a value set by the user (smaller than the size of the word set ) rather than the size of the word set , and has real values other than 0 and 1.\n",
    "- The method of expressing words as dense vectors is called word embedding, and the result obtained in this way is called an embedding vector.\n",
    "- Examples of word embeddings include, LSA, word2vec, FastText , and Glove.\n",
    "\n",
    "**◆ Distributed representation**\n",
    "- **Local representation** is a method of expressing a word by looking only at the word itself and mapping a specific value.\n",
    "- On the other hand , the distributed representation depends on the distribution hypothesis\n",
    "- Based on the expression, it is made on the assumption that words appearing in similar positions have similar meanings, and the task of vectorizing the similarity of words corresponds to word embedding.\n",
    "- Distributed representation methods refer to neighboring words to represent that word.\n",
    "- For example, since the words cute and lovely often appear near the word puppy, the word puppy defines the word as cute and lovely.\n",
    "- As an example of a distributed representation, There are techniques such as Word2vec."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d77818184524b2e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Topic Vector\n",
    "\n",
    "**◆ Topic vector (semantic vector)**\n",
    "- Dimensional reduction of multidimensional vectors whose components are subject scores obtained using the weighted frequencies of TF-IDF vectors\n",
    "- Group words of the same subject together using correlations between normalized term frequencies.\n",
    "- Used for semantic-based retrieval, which searches documents based on their semantics → usually than keyword-based search is known to be accurate.\n",
    "- Able to find a set of key words (keywords) that best summarize the meaning of a given document.\n",
    "- There are (1) word subject vectors representing the meaning of words and (2) document subject vectors representing the meaning of documents.\n",
    "\n",
    "▪ Word Topic Vectors : Create 3 topic scores {pet}, {animal } , {city} as subject vector reproduce\n",
    "\n",
    "<center>\n",
    "\n",
    "![topicvector.png](./images/TopicVector.png)\n",
    "\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b415710d36d5262b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "▪ Document subject vector\n",
    "To obtain a word2vec that contains the topic ( meaning ) of the entire document, the document topic vector is\n",
    "obtained as the sum of word vectors.\n",
    "\n",
    "▪ Word inference using subject vectors\n",
    "It can be converted to a word vector space with a lower dimension than the word frequency vector, and the word vector operation has meaning\n",
    "\n",
    "Useful for word analogy tasks\n",
    "\n",
    "Ex) king : male = female : ?\n",
    "\n",
    "    ➢ Topic : { male / female , adult / child , royal family / commoner }  \n",
    "    ➢ Words : King = { 1.0, 0.9, 0.9 }, Prince = {0.9, 0.1, 0.8}, Queen = { 0.1, 0.9, 0.8 }, Princess = {0.1, 0.1, 0.8}\n",
    "    male = { 1.0, 0.0, 0.0 }\n",
    "    female = { 0.1, 0.0, 0.0 }\n",
    "    king - male + female = { 0.1, 0.9, 0.9 } → close to queen"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72f74d956ce53d8f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Topic Vector Expansion : Word2Vec and BERT\n",
    "\n",
    "**◆ Word2vec (Tomasi Mikolov , MS Apprentice , 2012)**\n",
    "- Subject vectors are the same If it is only in a sentence, it has meaning as a word, but word2vec has meaning as a word nearby\n",
    "- n-gram consisting of words before and after the target word\n",
    "- Use window=k to specify\n",
    "- CBOW (Continuous Bag of Words) and Skip-Gram are available\n",
    "\n",
    "**◆ BERT: Bidirectional Encoder Representations from Transformer ( Google , 2018): Encoder only model**\n",
    "- Train the model using the encoder part of Pre-learning is performed using two language learning methods:\n",
    "mask language model and next sentence prediction.\n",
    "\n",
    "**◆ Difference between Word2vec and BERT**\n",
    "- Word2vec corresponds to the static embedding technique , so multiple meanings corresponding to one word are converted into only one vector.\n",
    "- A fixed expression and has the same expression value wherever it appears in the document text (regardless of order), so homophones cannot be distinguished.\n",
    "- As a solution to this, contextual embedding creates a dynamic vector for each context based on the sentence. The technique BERT, ELMo etc. are developed to solve this problem."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1494c2b5eb8a2d4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Integer Encoding\n",
    "\n",
    "**1. Dictionary**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69354c9da728ef81"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.131714Z",
     "start_time": "2023-11-06T07:42:30.037667Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/junghunlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "              \n",
    "raw_text = \"A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "tokenizing sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c9221826624883d"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A barber is a person.', 'a barber is good person.', 'a barber is huge person.', 'he Knew A Secret!', 'The Secret He Kept is huge secret.', 'Huge secret.', 'His barber kept his word.', 'a barber kept his word.', 'His barber kept his secret.', 'But keeping and keeping such a huge secret to himself was driving the barber crazy.', 'the barber went up a huge mountain.']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(raw_text)\n",
    "print(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.139487Z",
     "start_time": "2023-11-06T07:42:31.130262Z"
    }
   },
   "id": "ea2003ef037452bc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "preprocessed_sentences = []\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.145029Z",
     "start_time": "2023-11-06T07:42:31.139888Z"
    }
   },
   "id": "47dc90a54255cb0f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
      "word set : {'barber': 8, 'person': 3, 'good': 1, 'huge': 5, 'knew': 1, 'secret': 6, 'kept': 4, 'word': 2, 'keeping': 2, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1}\n",
      "Frequency of the word \"barber\": 8\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences : # tokenizing words\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    result = []\n",
    "\n",
    "    for word in tokenized_sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words and len(word) > 2:\n",
    "            result.append(word)\n",
    "            if word not in vocab:\n",
    "                vocab[word] = 0\n",
    "            vocab[word] += 1\n",
    "    \n",
    "    preprocessed_sentences.append(result)\n",
    "\n",
    "print(preprocessed_sentences)\n",
    "print('word set :', vocab)\n",
    "print('Frequency of the word \"barber\":', vocab[\"barber\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.153746Z",
     "start_time": "2023-11-06T07:42:31.148283Z"
    }
   },
   "id": "b5c9d6285d6a5be5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Build a dictionary based on frequencies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d77121f703674f4e"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3), ('word', 2), ('keeping', 2), ('good', 1), ('knew', 1), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)]\n"
     ]
    }
   ],
   "source": [
    "vocab_sorted = sorted( vocab.items (), key = lambda x:x[1], reverse = True)\n",
    "print(vocab_sorted)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.181258Z",
     "start_time": "2023-11-06T07:42:31.150662Z"
    }
   },
   "id": "4d04c74d936145b8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, 'keeping': 7}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab_sorted:\n",
    "    if frequency > 1 : # Exclude low frequency words\n",
    "        i = i + 1\n",
    "        word_to_index [word] = i # Index words based on frequency\n",
    "print( word_to_index )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.182939Z",
     "start_time": "2023-11-06T07:42:31.154179Z"
    }
   },
   "id": "1d97f69b9c44408"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove words with index greater than 5 (remove low frequency words)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab8ddb47c279f49e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "words_frequency = [word for word, index in word_to_index.items () if index >= vocab_size + 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.183324Z",
     "start_time": "2023-11-06T07:42:31.157231Z"
    }
   },
   "id": "c07b6503c1a3a40b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Delete the index information for the word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "482afb7910cf2bbb"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "for w in words_frequency :\n",
    "    del word_to_index[w]\n",
    "print(word_to_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.192643Z",
     "start_time": "2023-11-06T07:42:31.159433Z"
    }
   },
   "id": "537e7e1df4151e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Words with an index greater than 5 ( low frequency ) are collectively referred to as"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ff033ae961fc2f4a"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'OOV': 6}\n"
     ]
    }
   ],
   "source": [
    "word_to_index ['OOV'] = len ( word_to_index ) + 1\n",
    "print(word_to_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.193618Z",
     "start_time": "2023-11-06T07:42:31.162712Z"
    }
   },
   "id": "71c1cbd2febccecc"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 6, 3, 6], [1, 6, 3, 6], [1, 6, 3, 6], [1, 6, 3, 6]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sentences = []\n",
    "\n",
    "for sentence in preprocessed_sentences :\n",
    "    encoded_sentence = []\n",
    "    \n",
    "for word in sentence:\n",
    "    \n",
    "    try: # If a word is in the word set, return the integer for that word\n",
    "        encoded_sentence.append(word_to_index[word])\n",
    "        \n",
    "    except KeyError : # If the word is not in the word set, return an integer of\n",
    "        encoded_sentence.append(word_to_index['OOV'])\n",
    "        \n",
    "    encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "print(encoded_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.194501Z",
     "start_time": "2023-11-06T07:42:31.167858Z"
    }
   },
   "id": "5971f5592cd06705"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Counting**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c23629a607f941c2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(preprocessed_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.195219Z",
     "start_time": "2023-11-06T07:42:31.170525Z"
    }
   },
   "id": "efec595fde4637ab"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barber', 'person', 'barber', 'good', 'person', 'barber', 'huge', 'person', 'knew', 'secret', 'secret', 'kept', 'huge', 'secret', 'huge', 'secret', 'barber', 'kept', 'word', 'barber', 'kept', 'word', 'barber', 'kept', 'secret', 'keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy', 'barber', 'went', 'huge', 'mountain']\n"
     ]
    }
   ],
   "source": [
    "# words = np.hstack(preprocessed_sentences) can also be done\n",
    "all_words_list = sum(preprocessed_sentences, [])\n",
    "print(all_words_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.195670Z",
     "start_time": "2023-11-06T07:42:31.174053Z"
    }
   },
   "id": "4c04d2fe416b28bb"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'barber': 8, 'secret': 6, 'huge': 5, 'kept': 4, 'person': 3, 'word': 2, 'keeping': 2, 'good': 1, 'knew': 1, 'driving': 1, 'crazy': 1, 'went': 1, 'mountain': 1})\n"
     ]
    }
   ],
   "source": [
    "# Python's Count the frequency of words using the Counter module\n",
    "vocab = Counter(all_words_list)\n",
    "print(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.196016Z",
     "start_time": "2023-11-06T07:42:31.176789Z"
    }
   },
   "id": "65c9558be1287688"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(vocab['barber'])  # print the frequency of the word 'barber'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.196201Z",
     "start_time": "2023-11-06T07:42:31.179484Z"
    }
   },
   "id": "378883079d5ab505"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # Store only the top 5 most frequent words\n",
    "word_to_index = {}\n",
    "i = 0\n",
    "for (word, frequency) in vocab:\n",
    "    i = i + 1\n",
    "    word_to_index [word] = i\n",
    "print(word_to_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.204637Z",
     "start_time": "2023-11-06T07:42:31.182445Z"
    }
   },
   "id": "32a476e82f8cad90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. NLTK's FreqDist**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb86352436da6f07"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.205340Z",
     "start_time": "2023-11-06T07:42:31.185144Z"
    }
   },
   "id": "91515b81cdc4be1e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove sentence breaks with np.hstack"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9b9eb0f36adbd2a"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "vocab = FreqDist(np.hstack(preprocessed_sentences))\n",
    "print(vocab[\"barber\"]) # print the frequency of the word 'barber’"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.205845Z",
     "start_time": "2023-11-06T07:42:31.188056Z"
    }
   },
   "id": "e1ae6a0012f81dba"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('barber', 8), ('secret', 6), ('huge', 5), ('kept', 4), ('person', 3)]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "vocab = vocab.most_common(vocab_size) # Store only the top 5 most frequent words\n",
    "print(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.206384Z",
     "start_time": "2023-11-06T07:42:31.191481Z"
    }
   },
   "id": "4a5e1502117ae531"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word[0] : index + 1 for index, word in enumerate(vocab)}\n",
    "print(word_to_index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:42:31.209920Z",
     "start_time": "2023-11-06T07:42:31.194639Z"
    }
   },
   "id": "c313db1ffe04e75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**4. Understanding enumerate**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28f85847c248d9a2"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value : a, index : 0\n",
      "value : b, index : 1\n",
      "value : c, index : 2\n",
      "value : d, index : 3\n",
      "value : e, index : 4\n"
     ]
    }
   ],
   "source": [
    "test_input = ['a','b','c','d','e']\n",
    "for index, value in enumerate(test_input) :\n",
    "    print(f\"value : {value}, index : {index}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:44:00.019728Z",
     "start_time": "2023-11-06T07:43:59.986610Z"
    }
   },
   "id": "7a4c45d5bebef7bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**5. Keras Text Preprocessing**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7d91b2e488cb57f"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept ', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "\n",
    "tokenizer = Tokenizer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:45:40.291929Z",
     "start_time": "2023-11-06T07:45:40.281244Z"
    }
   },
   "id": "6b9d0bd343640ab9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "input corpus into"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20dbb3d283327fd6"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'person': 4, 'kept': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'kept ': 10, 'driving': 11, 'crazy': 12, 'went': 13, 'mountain': 14}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 3), ('word', 2), ('kept ', 1), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
      "[[1, 4], [1, 8, 4], [1, 3, 4], [9, 2], [2, 5, 3, 2], [3, 2], [1, 5, 6], [1, 10, 6], [1, 5, 2], [7, 7, 3, 2, 11, 1, 12], [1, 13, 3, 14]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:46:39.691888Z",
     "start_time": "2023-11-06T07:46:39.656639Z"
    }
   },
   "id": "d7111eab5df4d673"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'person': 4, 'kept': 5, 'word': 6, 'keeping': 7, 'good': 8, 'knew': 9, 'kept ': 10, 'driving': 11, 'crazy': 12, 'went': 13, 'mountain': 14}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('good', 1), ('huge', 5), ('knew', 1), ('secret', 6), ('kept', 3), ('word', 2), ('kept ', 1), ('keeping', 2), ('driving', 1), ('crazy', 1), ('went', 1), ('mountain', 1)])\n",
      "[[1, 4], [1, 4], [1, 3, 4], [2], [2, 5, 3, 2], [3, 2], [1, 5], [1], [1, 5, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 1) # 상위 5개 단어만 사용\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:46:46.454501Z",
     "start_time": "2023-11-06T07:46:46.424583Z"
    }
   },
   "id": "3e2ca070cfc775f"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "vocab_size = 5\n",
    "words_frequency = [word for word , index in tokenizer.word_index.items( ) if index >= vocab_size + 1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:47:51.950385Z",
     "start_time": "2023-11-06T07:47:51.928090Z"
    }
   },
   "id": "676f7cd1867bf8d7"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'barber': 1, 'secret': 2, 'huge': 3, 'person': 4, 'kept': 5}\n",
      "OrderedDict([('barber', 8), ('person', 3), ('huge', 5), ('secret', 6), ('kept', 3)])\n",
      "[[1, 4], [1, 4], [1, 3, 4], [2], [2, 5, 3, 2], [3, 2], [1, 5], [1], [1, 5, 2], [3, 2, 1], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "# delete cases with more than 5 ubdexes\n",
    "for word in words_frequency :\n",
    "    del tokenizer.word_index[word] # delete index information for that word\n",
    "    del tokenizer.word_counts[word] # delete count information for that word\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.word_counts)\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:48:19.660238Z",
     "start_time": "2023-11-06T07:48:19.629592Z"
    }
   },
   "id": "f8b82a4beb1491ec"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token OOV value : 1\n",
      "[[2, 5], [2, 1, 5], [2, 4, 5], [1, 3], [3, 6, 4, 3], [4, 3], [2, 6, 1], [2, 1, 1], [2, 6, 3], [1, 1, 4, 3, 1, 2, 1], [2, 1, 4, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Size of word set is +2 , taking into account the number 0 and OOV\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words = vocab_size + 2, oov_token = 'OOV');\n",
    "tokenizer.fit_on_texts(preprocessed_sentences);\n",
    "\n",
    "print('Token OOV value : {}'.format(tokenizer.word_index['OOV']))\n",
    "print(tokenizer.texts_to_sequences(preprocessed_sentences));"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:49:25.566136Z",
     "start_time": "2023-11-06T07:49:25.532189Z"
    }
   },
   "id": "f1102336be6ec082"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Padding\n",
    "\n",
    "**1. Padding using Numpy**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7bbd1b4535ce55b"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "preprocessed_sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'], ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'], ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(preprocessed_sentences)\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:50:41.172381Z",
     "start_time": "2023-11-06T07:50:41.138175Z"
    }
   },
   "id": "e4bd0f0b2f99fe0e"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이 : 7\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(item) for item in encoded)\n",
    "print('최대 길이 :',max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:50:50.123108Z",
     "start_time": "2023-11-06T07:50:50.087984Z"
    }
   },
   "id": "a66629b2e41f72b1"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1,  5,  0,  0,  0,  0,  0],\n       [ 1,  8,  5,  0,  0,  0,  0],\n       [ 1,  3,  5,  0,  0,  0,  0],\n       [ 9,  2,  0,  0,  0,  0,  0],\n       [ 2,  4,  3,  2,  0,  0,  0],\n       [ 3,  2,  0,  0,  0,  0,  0],\n       [ 1,  4,  6,  0,  0,  0,  0],\n       [ 1,  4,  6,  0,  0,  0,  0],\n       [ 1,  4,  2,  0,  0,  0,  0],\n       [ 7,  7,  3,  2, 10,  1, 11],\n       [ 1, 12,  3, 13,  0,  0,  0]])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sentence in encoded:\n",
    "    while len(sentence) < max_len:\n",
    "        sentence.append(0)\n",
    "padded_np = np.array(encoded)\n",
    "padded_np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:50:57.365823Z",
     "start_time": "2023-11-06T07:50:57.337871Z"
    }
   },
   "id": "28a64ea42d7a86d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Padding using Keras tools** "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf5475fb05697ff"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "encoded = tokenizer.texts_to_sequences(preprocessed_sentences)\n",
    "print(encoded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:52:16.341567Z",
     "start_time": "2023-11-06T07:52:16.311077Z"
    }
   },
   "id": "f6af9e52a30842a7"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(encoded)\n",
    "padded = pad_sequences(encoded, padding = 'post')\n",
    "(padded == padded_np).all()\n",
    "padded = pad_sequences(encoded, padding='post', maxlen =5)\n",
    "padded = pad_sequences(encoded, padding='post', truncating='post', maxlen =5)\n",
    "last_value = len(tokenizer.word_index) + 1 # use a number one greater than the size of the word set\n",
    "print(last_value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:52:39.312366Z",
     "start_time": "2023-11-06T07:52:39.277493Z"
    }
   },
   "id": "d5381a87f8d12a15"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "padded = pad_sequences(encoded, padding = 'post', value = last_value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:52:49.695760Z",
     "start_time": "2023-11-06T07:52:49.667142Z"
    }
   },
   "id": "5ebbb852d26fb3c0"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5 14 14 14 14 14]\n",
      " [ 1  8  5 14 14 14 14]\n",
      " [ 1  3  5 14 14 14 14]\n",
      " [ 9  2 14 14 14 14 14]\n",
      " [ 2  4  3  2 14 14 14]\n",
      " [ 3  2 14 14 14 14 14]\n",
      " [ 1  4  6 14 14 14 14]\n",
      " [ 1  4  6 14 14 14 14]\n",
      " [ 1  4  2 14 14 14 14]\n",
      " [ 7  7  3  2 10  1 11]\n",
      " [ 1 12  3 13 14 14 14]]\n"
     ]
    }
   ],
   "source": [
    "print(padded)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T07:52:58.606368Z",
     "start_time": "2023-11-06T07:52:58.592014Z"
    }
   },
   "id": "5f8904a4e2483f29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "eb6bd0d809b9880f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
