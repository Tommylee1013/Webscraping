{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Text Data Structure\n",
    "\n",
    "#### Word Expreassion\n",
    "\n",
    "**◆ word vectors (word representations)**\n",
    "- The most basic problem of natural language processing is how to make a computer recognize natural language.\n",
    "- Computer recognizes natural language as binary code ( Unicode , ASCII code ,…). \n",
    "    Ex) English : 1100010110111000, English : 1100010110110100\n",
    "- This way of expression has no characteristics of words at all.\n",
    "- It can be used for classification and clustering.\n",
    "\n",
    "**◆ One-Hot encoding (one-hot encoding)**\n",
    "\n",
    "words It is expressed as a single vector , with only one 1 at a specific position, and the rest are marked as 0. Ex) {Thomas Jepperson made Jepperson building}\n",
    "\n",
    "<center>\n",
    "\n",
    "||Thomas|Jepperson|made|building|\n",
    "|:---:|:---:|:---:|:---:|:---:|\n",
    "|Thomas|1|0|0|0|\n",
    "|Japperson|0|1|0|0|\n",
    "|made|0|0|1|0|\n",
    "|Japperson|0|1|0|0|\n",
    "|building|0|0|0|1|\n",
    "\n",
    "</center>\n",
    "\n",
    "- To know what the nth word is in a row ( sentence ) , you need to know the column value that is 1 in that row. (100% restorable)\n",
    "- One row is each binary row vector, where only one is 1 and the rest are 0.\n",
    "- Columns act as a dictionary of words (terms)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "81be61005e2b801"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ One-hot Disadvantages and Alternatives**\n",
    "\n",
    "- one - hot The disadvantage of encoding is that it becomes inefficient because the size of the vector becomes\n",
    "large when there are many words.\n",
    "- To overcome various disadvantages, two alternatives are proposed.\n",
    "\n",
    "**◆ Two alternatives**\n",
    "1. frequency information based\n",
    "\n",
    "    a. word frequency vector (bag of words)\n",
    "   \n",
    "    b. word - document matrix method (TF-IDF , etc. )\n",
    "   \n",
    "    c. co-occurrence matrix : word - word matrix , document - document matrix\n",
    "   \n",
    "2. Meaning ( subject / characteristic ) information-based\n",
    "\n",
    "    a. subject vector ( semantic vector )\n",
    "   \n",
    "    b. word2vec/ Glove ( method is different, but the solution is the same )\n",
    "   \n",
    "    c. BERT, GPT, ...\n",
    "    \n",
    "**◆ Word frequency vector (word collection vector): Bag of words**\n",
    "- A method of trying to understand the meaning of a sentence only with word collection data (word frequency), ignoring the order and grammar of words in a sentence\n",
    "- Unlike the one-hot vector, it contains only the number of appearances, so it is difficult to reproduce the document .\n",
    "- In the most basic way, there is a vector of binary word collections .\n",
    "- Binary word collection vectors are useful for document search indexing, which tells which word is used in which document ."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee1974da398397a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Integer Encoding & Padding\n",
    "\n",
    "**◆ Integer encoding**\n",
    "- It is the basic step among several techniques for converting text to numbers in natural language processing.\n",
    "- A preprocessing task that maps each word to a unique integer.\n",
    "- If there are 5,000 words in the text, unique integers mapped to words from 1 to 5,000 in each of the 5,000 words, In other words, an index is given, usually after sorting by word frequency.\n",
    "- One of the ways to assign integers to words is to create a set of words (vocabulary) in which words are sorted in order of frequency. There is a way to assign integers from lowest to highest in order\n",
    "\n",
    "**◆ padding**\n",
    "- Each sentence ( or document ) can be of different lengths , but the machine divides all documents of the same length into one matrix. Reports can be grouped together and processed .\n",
    "- Arbitrarily equalizing the length of several sentences for parallel operation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9befed24f2c2226"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Word document procession\n",
    "\n",
    "**◆ word Frequency (Term Frequency: TF)**\n",
    "- the number of times the word appeared in the document\n",
    "- If a particular word appears frequently in a particular document, the word is said to be closely related to that document.\n",
    "\n",
    "```\n",
    "// Example\n",
    "\n",
    "Doc1 : the fox chases the rabbit\n",
    "Doc2 : the rabbit ate the cabbage\n",
    "Doc3 : the fox caught the rabbit\n",
    "```\n",
    "\n",
    "Rows are words, columns are documents(TDM: Term-Document Matrix)\n",
    "\n",
    "<center>\n",
    "\n",
    "|          | Doc1 | Doc2 | Doc3 |\n",
    "|:--------:|:----:|:----:|:----:|\n",
    "|   the    |  2   |  2   |  2   |\n",
    "|   fox    |  1   |  0   |  1   |\n",
    "|  rabbit  |  1   |  1   |  1   |\n",
    "|  chases  |  1   |  0   |  0   |\n",
    "|  caught  |  0   |  0   |  1   |\n",
    "| cabbage  |  0   |  1   |  0   |\n",
    "|   ate    |  0   |  1   |  0   |\n",
    "\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34e2f8d014190e42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Word document matrix\n",
    "\n",
    "**◆ word frequency reverse document frequency**\n",
    "- **Zipf's Law** : The frequency of use of any word is inversely proportional to the rank of that word. (Ex: 1st place is 3 times as frequent as 3rd place)\n",
    "- Give low weight to words that appear frequently in the document but do not help to understand the meaning of the document -> IDF\n",
    "\n",
    "\n",
    "**IDF**\n",
    "\n",
    "A weight that measures the importance of a word. \n",
    "\n",
    "$$\\mathrm{IDF} = \\log(\\frac{\\mathrm{N}}{\\mathrm{DF}})$$\n",
    "\n",
    "N is the total number of documents. DF is the document frequency (the number of documents in which the word appears)\n",
    "\n",
    "- The smaller the DF, the higher the importance of the word.\n",
    "- The higher the IDF, the higher the importance of the word\n",
    "- Words with high TF–IDF values give high discrimination in documents (important words in information retrieval)\n",
    "- Calculate TF-IDF : (TF-IDF)(t, d)=TF(t, d) x IDF(t), where t is the word and d is the document\n",
    "\n",
    "<center>\n",
    "\n",
    "|          | Doc1 | Doc2 | Doc3 | DF | N/DF | IDF=$\\log_2(\\mathrm{N}/\\mathrm{DF})$ |\n",
    "|:--------:|:----:|:----:|:----:|:--:|:----:|:------------------------------------:| \n",
    "|   the    |  2   |  2   |  2   | 3  | 3/3  |            $\\log_2(3/3)$             |\n",
    "|   fox    |  1   |  0   |  1   | 2  | 3/2  |            $\\log_2(3/2)$             |\n",
    "|  rabbit  |  1   |  1   |  1   | 3  | 3/3  |            $\\log_2(3/3)$             |\n",
    "|  chases  |  1   |  0   |  0   | 1  | 3/1  |            $\\log_2(3/1)$             |\n",
    "|  caught  |  0   |  0   |  1   | 1  | 3/1  |            $\\log_2(3/1)$             |\n",
    "| cabbage  |  0   |  1   |  0   | 1  | 3/1  |            $\\log_2(3/1)$             |\n",
    "|   ate    |  0   |  1   |  0   | 1  | 3/1  |            $\\log_2(3/1)$             |\n",
    "\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f39116af5182174b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ TF standardization and regularization**\n",
    "- The longer the length of the document, the higher the frequency of occurrence of the word and\n",
    "the higher the possibility of being searched.\n",
    "- Thus the longer the length of the document, the higher the possibility of similarity with other documents .\n",
    "- Standardization and normalization of TF is necessary to complicate these week-points.\n",
    "\n",
    "**◆ Standardization** \n",
    "\n",
    "$$z = \\frac{\\mathrm{TF}-\\mu(\\mathrm{TF})}{\\sigma(\\mathrm{TF})}$$\n",
    "\n",
    "Example) doc 1\n",
    "\n",
    "$$\\mu(\\mathrm{TF}) = \\frac{5}{7} ~~ (\\frac{\\mathrm{number~of~occurrences}}{\\mathrm{total~number~of~words}})$$\n",
    "$$\\sigma(\\mathrm{TF}) = \\sqrt{\\frac{(2 - \\mu)^2 + 3(1 - \\mu)^2 + 3(0-\\mu)^2}{6}}$$\n",
    "\n",
    "<center>\n",
    "\n",
    "|          |   Doc1    |   Doc2    |    Doc3    |\n",
    "|:--------:|:---------:|:---------:|:----------:|\n",
    "|   the    |  1.70084  |  1.70084  |  1.70084   |\n",
    "|   fox    |  0.37796  | -0.944911 |  0.37796   |\n",
    "|  rabbit  |  0.37796  |  0.37796  |  0.37796   |\n",
    "|  chases  |  0.37796  | -0.944911 | -0.944911  |\n",
    "|  caught  | -0.944911 | -0.944911 |  0.37796   |\n",
    "| cabbage  | -0.944911 |  0.37796  | -0.944911  |\n",
    "|   ate    | -0.944911 |  0.37796  | -0.944911  |\n",
    "\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aec89c5d30c31b7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ Normalization**\n",
    "\n",
    "divide TF by the total frequency of the word (1+log(TF))/ ni\n",
    "\n",
    "ni : frequency count of total words in\n",
    "\n",
    "<center>\n",
    "\n",
    "|          |   Doc1    |   Doc2    | Doc3 |\n",
    "|:--------:|:---------:|:---------:|:----:|\n",
    "|   the    |    0.4    |    0.4    | 0.4  |\n",
    "|   fox    |    0.2    |     0     | 0.2  |\n",
    "|  rabbit  |    0.2    |    0.2    | 0.2  |\n",
    "|  chases  |    0.2    |     0     |  0   |\n",
    "|  caught  |     0     |     0     | 0.2  |\n",
    "| cabbage  |     0     |    0.2    |  0   |\n",
    "|   ate    |     0     |    0.2    |  0   |\n",
    "\n",
    "</center>\n",
    "\n",
    "where $0.4 = \\frac{1 + \\log_2{2}}{5}$ and $0.2 = \\frac{1 + \\log_2{1}}{5}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "572953d417ff4d15"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ Normalized TF-IDF: Normalized TF times IDF**\n",
    "\n",
    "<center>\n",
    "\n",
    "| Normalized TF-IDF |                Doc1                |             Doc2             |  Doc3   |\n",
    "|:-----------------:|:----------------------------------:|:----------------------------:|:-------:|\n",
    "|        the        |    0 = $0.4 \\times \\log_2(3/3)$    | 0 = $0.4 \\times \\log_2(3/3)$ |    0    |\n",
    "|        fox        | 0.11699 = $0.2 \\times \\log_2(3/2)$ |              0               | 0.11699 |\n",
    "|      rabbit       |    0 = $0.2 \\times \\log_2(3/3)$    |              0               |    0    |\n",
    "|      chases       | 0.31699 = $0.2 \\times \\log_2(3/1)$ |              0               |    0    |\n",
    "|      caught       |                 0                  |              0               | 0.31699 |\n",
    "|      cabbage      |                 0                  |           0.31699            |    0    |\n",
    "|        ate        |                 0                  |           0.31699            |    0    |\n",
    "\n",
    "</center>\n",
    "\n",
    "**◆ Disadvantages of**\n",
    "- Vectors of two pieces of text with different words, even though they have similar meanings (subjects), are in the TF-IDF vector space. If words with the same meaning but different spellings, TF-IDF vectors do not lie close together in the vector space.\n",
    "\n",
    "**The TF-IDF method is difficult to use in the process of finding documents that are similar in meaning (topic)**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7890e1b1f354696c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Co-Occurrence Matrix\n",
    "\n",
    "**◆ joint ( simultaneous ) occurrence matrix**\n",
    "\n",
    "A method of directly counting the number of times words appear simultaneously in a particular context. The number of simultaneous appearances is expressed as a matrix and the matrix is digitized to create word vectors.\n",
    "\n",
    "```\n",
    "Ex) \n",
    "Myeong-seok and Jun-seon went to America\n",
    "Myeong-seok and Sang-ho went to the library\n",
    "Myeong-seok and Jun-seon like cold noodles\n",
    "```\n",
    "\n",
    "< Co-occurrence matrix : word - word matrix > → square matrix , symmetric matrix\n",
    "\n",
    "<center>\n",
    "\n",
    "![wordvector](./images/wordvector.png)\n",
    "\n",
    "</center>\n",
    "\n",
    "→ Can be used as a social network analysis (e.g., calculating the centrality of each word (degree of connection , proximity , median , eigenvector))\n",
    "\n",
    "- TDM : Term based Matrix\n",
    "\n",
    "- DTM : Document based Matrix"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "345dd434cd45d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Word Embedding\n",
    "\n",
    "**◆ Word embedding (word embedding)**\n",
    "- A one-hot vector is a sparse representation with many 0 's and only one 1’.\n",
    "- In contrast to sparse representation , the size of the vector is determined by a value set by the user (smaller than the size of the word set ) rather than the size of the word set , and has real values other than 0 and 1.\n",
    "- The method of expressing words as dense vectors is called word embedding, and the result obtained in this way is called an embedding vector.\n",
    "- Examples of word embeddings include, LSA, word2vec, FastText , and Glove.\n",
    "\n",
    "**◆ Distributed representation**\n",
    "- **Local representation** is a method of expressing a word by looking only at the word itself and mapping a specific value.\n",
    "- On the other hand , the distributed representation depends on the distribution hypothesis\n",
    "- Based on the expression, it is made on the assumption that words appearing in similar positions have similar meanings, and the task of vectorizing the similarity of words corresponds to word embedding.\n",
    "- Distributed representation methods refer to neighboring words to represent that word.\n",
    "- For example, since the words cute and lovely often appear near the word puppy, the word puppy defines the word as cute and lovely.\n",
    "- As an example of a distributed representation, There are techniques such as Word2vec."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d77818184524b2e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Topic Vector\n",
    "\n",
    "**◆ Topic vector (semantic vector)**\n",
    "- Dimensional reduction of multidimensional vectors whose components are subject scores obtained using the weighted frequencies of TF-IDF vectors\n",
    "- Group words of the same subject together using correlations between normalized term frequencies.\n",
    "- Used for semantic-based retrieval, which searches documents based on their semantics → usually than keyword-based search is known to be accurate.\n",
    "- Able to find a set of key words (keywords) that best summarize the meaning of a given document.\n",
    "- There are (1) word subject vectors representing the meaning of words and (2) document subject vectors representing the meaning of documents.\n",
    "\n",
    "▪ Word Topic Vectors : Create 3 topic scores {pet}, {animal } , {city} as subject vector reproduce\n",
    "\n",
    "<center>\n",
    "\n",
    "![topicvector.png](./images/TopicVector.png)\n",
    "\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b415710d36d5262b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
