{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Deep Learning Language Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "46847f06330bbf1e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Language Model\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{P(B)}$$\n",
    "\n",
    "**◆ Language model basics**\n",
    "- A model that assigns probabilities to word sequences (arrangements of words).\n",
    "- The probability of a whole sequence of words is equal to the chain of probabilities of the next word given the previous words. A language model is a model that assigns the probability of the next word appearing given the previous words\n",
    "\n",
    "$$P(a1, a2, a3) = P(a1) \\times P(a2|a1) \\times P(a3|a1, a2)$$\n",
    "$$P(\\mbox{I, Sogang University, I am a student}) = P(\\mbox{I}) \\times P(\\mbox{Sogang University| I}) \\times P(\\mbox{I am a student|I, Sogang University})$$\n",
    "\n",
    "**◆ Language model evolution**\n",
    "- N-gram model: Calculate the probability for each word by the frequency of the preceding word -> context is not considered\n",
    "- Word2Vec-based model: Deep learning-based model trained with perceptron. Expressing word-to-word relationships -> no consideration of context\n",
    "-  RNN-based model : It has a recursive structure that processes each element of the sequence sequentially and uses previous information for the current calculation. As the length of the input sequence increases, it is difficult to maintain long-term dependency on information (long-term dependency problem). Modified RNN structures such as LSTM and GRU have emerged, but performance deteriorates as sentences become longer.\n",
    "- Attention-based model: Attention vectors are generated and applied\n",
    "- Transformer-based model: Self-attention is used\n",
    "    - Reason \n",
    "        - (1) Operation per layer \n",
    "        - (2) Parallel processing\n",
    "        - (3) Dependency learning between distant words \n",
    "    - Ex) BERT, GPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "975cd22ecefb2e91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ Difference between RNN-based model and attention-based model**\n",
    "\n",
    "1. Sequence processing method:\n",
    "    Attention: Processes all elements of the input sequence simultaneously and calculates the relevance between each element. Each element obtains the processing result through a weighted sum.\n",
    "    RNN: Processes the sequence step by step, passing information from the previous step to the current step. Calculations are performed sequentially based on previous information, and the hidden state serves to maintain previous information.\n",
    "2. Handling long-term dependencies:\n",
    "    Attention: Calculates the relevance between each element within the input sequence to further highlight and focus important information.\n",
    "    RNN: Difficulty handling long-term dependencies. Information loss and gradient vanishing problems occur in long sequences\n",
    "3. Parallel processing:\n",
    "    Attention: Advantageous for parallel processing because the relevance between each element can be calculated in parallel\n",
    "    RNN: Difficult to parallelize because it is calculated sequentially, and processing speed is slow on large data\n",
    "4. Sequence length handling:\n",
    "    Attention: processed in the same way regardless of the length of the input sequence\n",
    "    RNN: The disadvantage is that the longer the input sequence, the more difficult it is to maintain information.\n",
    "5. Memory requirements:\n",
    "    Attention: Memory requirements may be high because information within the input sequence is stored in the form of a weighted sum.\n",
    "    RNN: Because it is computed sequentially, memory requirements can be relatively low."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94d8e7b2a7aeab34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Transformer\n",
    "\n",
    "<center>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png\" alt=\"My Image\"> </center>\n",
    "\n",
    "**◆ Transformer Model Summary**\n",
    "- Transformer has a sequence-to-sequence model structure proposed by Google in 2017 and is widely used in machine translation.\n",
    "- It is also widely used for positive and negative classification of review articles.\n",
    "- Reduced learning time by not using LSTM or RNN\n",
    "- In the case of a positive or negative decision, the word with strong attention applied is visualized to show the basis for the decision (explainable artificial intelligence)\n",
    "- Possible to express a word vector depending on the context: Each word has various meanings, but it can have different meanings depending on the context\n",
    "- An encoder-decoder model. When an input sentence (original text) is input to the encoder, the encoder learns how to express the input sentence and sends the result to the decoder. The decoder receives the expression result learned from the encoder and generates the desired sentence.\n",
    "\n",
    "**◆ Transformers model structure (e.g. machine translation)**\n",
    "- After tokenizing with a tokenizer, each token is mapped to a number\n",
    "- The tokenized input becomes the input of the transformer, and the transformer uses the input value to output a token and reverse the token to complete the sentence.\n",
    "- Encoder and decoder are composed of 6 sub-layers each\n",
    "- It is connected in a row to the encoder sublayer and input values are passed in order, but the decoder sublayer receives the output of the previous sublayer and the last output value of the encoder as input.\n",
    "- The sub-layer structure of the encoder and decoder is shown in the right figure.\n",
    "\n",
    "<center>\n",
    "\n",
    "![cluster](./images/deeplearning.png) \n",
    "\n",
    "</center>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b56466747bf3090"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**◆ Sequence to Sequence (Seq2Seq) Structure**\n",
    "- sequence : a sequence of words\n",
    "- Sequence-to-Sequence : Converting a sequence with one property into a sequence with another property.\n",
    "- ex ) 어제, 에버랜드, 갔었어, 거기, 사람, 많더라  $\\Rightarrow$ I, went, to, the, ever-land, there, were, many, people, there\n",
    "\n",
    "**◆ Encoder and decoder**\n",
    "- A model that performs a sequence-to-sequence task consists of an encoder and a decoder.\n",
    "- The encoder is responsible for compressing the source sequence information and sending it to the decoder.\n",
    "- Encoding: The process of compressing source sequence information\n",
    "- Decoding: The process of generating a target sequence by receiving information sent by the encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "466a62ce9234a092"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
