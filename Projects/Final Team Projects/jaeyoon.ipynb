{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, NoAlertPresentException\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify your Naver Cafe URL, username, and password\n",
    "cafe_url = 'https://cafe.naver.com/laterlife'\n",
    "username = 'lucas4569'\n",
    "password = 'Sogang0217@'\n",
    "\n",
    "# Set up Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def naver_login(driver, username, password):\n",
    "    driver.get('https://nid.naver.com/nidlogin.login')\n",
    "    driver.execute_script(\"document.getElementsByName('id')[0].value='\" + username + \"'\")\n",
    "    driver.execute_script(\"document.getElementsByName('pw')[0].value='\" + password + \"'\")\n",
    "    driver.find_element(by=By.XPATH, value='//*[@id=\"log.login\"]').click()\n",
    "    time.sleep(1)\n",
    "\n",
    "def extract_content(url):\n",
    "    내용 = []\n",
    "\n",
    "    try:\n",
    "        # Use WebDriverWait for the page content to load\n",
    "        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((By.NAME, \"cafe_main\")))\n",
    "\n",
    "        # Wait for the content elements to be present\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.se-component.se-text.se-l-default')))\n",
    "\n",
    "        # Find all elements with the specified class\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Use find_all to get all elements with the specified class\n",
    "        contents_elements = soup.find_all('div', class_='se-component se-text se-l-default')\n",
    "\n",
    "        # Iterate through elements to get text content\n",
    "        for element in contents_elements:\n",
    "            try:\n",
    "                # Extract text content and append to the list\n",
    "                text_content = ' '.join(part.get_text(strip=True) for part in element.find_all(['p', 'span']))\n",
    "                내용.append(text_content)\n",
    "            except AttributeError:\n",
    "                print(f\"No text data found for URL: {url}. Skipping to the next page.\")\n",
    "                break\n",
    "\n",
    "    except NoAlertPresentException:\n",
    "        print(f\"No alert present for URL: {url}. Skipping to the next page.\")\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout occurred while processing URL: {url}. Skipping to the next page.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing URL: {url}. Error: {e}\")\n",
    "    finally:\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    return 내용\n",
    "\n",
    "def save_to_excel(content_list):\n",
    "    df = pd.DataFrame({\"내용\": content_list})\n",
    "    df.to_excel(\"test3.xlsx\", index=False)\n",
    "\n",
    "# Log in to Naver\n",
    "naver_login(driver, username, password)\n",
    "\n",
    "# Extract URLs from the cafe\n",
    "주소 = []\n",
    "\n",
    "for i in range(401, 500 + 1):  # Adjusted the range for the next set of pages\n",
    "    url_string = f'{cafe_url}?iframe_url=/ArticleList.nhn%3Fsearch.clubid=23676262%26search.boardtype=L%26search.totalCount=151%26search.cafeId=20365296%26search.page={i}'\n",
    "\n",
    "    driver.get(url_string)\n",
    "    time.sleep(0.2)\n",
    "    driver.switch_to.frame(\"cafe_main\")\n",
    "    \n",
    "    for j in range(1, 16, 1):\n",
    "        xpath = f\"//div[@class='article-board m-tcol-c']/table/tbody/tr[{j}]/td[1]/div[2]/div/a\"\n",
    "        try:\n",
    "            url_element = driver.find_element(By.XPATH, xpath)\n",
    "            url = url_element.get_attribute(\"href\")\n",
    "            주소.append(url)\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Element not found for XPath: {xpath}. Skipping to the next row.\")\n",
    "\n",
    "    # Navigate to the next page by clicking on the \"다음\" (Next) button\n",
    "    try:\n",
    "        next_page_button = driver.find_element(By.CLASS_NAME, 'pgR')\n",
    "        next_page_button.click()\n",
    "        time.sleep(0.2)  # Add a short delay to allow the page to load\n",
    "    except NoSuchElementException:\n",
    "        print(\"Next page button not found. Exiting loop.\")\n",
    "\n",
    "# Perform actions on the extracted URLs and save to Excel\n",
    "내용 = []\n",
    "\n",
    "for url in 주소:\n",
    "    driver.get(url)\n",
    "    내용.extend(extract_content(url))\n",
    "\n",
    "# Save to Excel\n",
    "save_to_excel(내용)\n",
    "\n",
    "# Close the browser window\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
